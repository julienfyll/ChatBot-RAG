# .env
    
# --- CONFIGURATION LLM (Specifique à ta machine) ---

# Chemin vers l'exécutable llama-server (modifie-le si tu bouges le dossier)
LLM_BINARY_PATH="/home/fayolle/workspace/llama.cpp/build/bin/llama-server"

# Chemin vers le modèle (relatif ou absolu)
LLM_MODEL_PATH="models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# --- PARAMÈTRES RÉSEAU & GPU ---
LLM_HOST="127.0.0.1"
LLM_PORT=8080

# Taille du contexte (Important pour le RAG : 8192 ou 16384)
LLM_CONTEXT_SIZE=8192

# Nombre de couches sur le GPU (99 = tout, 0 = tout CPU)
LLM_GPU_LAYERS=99

# --- CONFIGURATION EMBEDDING MODEL (Specifique à ta machine) ---

# EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  