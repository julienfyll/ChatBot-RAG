# #!/bin/bash
# # Script de configuration ET de lancement pour le projet RAG (Linux/macOS)

# # Arr√™te le script si une commande √©choue
# set -e

# # --- D√©finition des constantes du projet ---
# ENV_NAME="ragcdl"
# MODEL_DIR="models"
# MODEL_NAME="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
# MODEL_PATH="$MODEL_DIR/$MODEL_NAME"
# MODEL_URL="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
# BINARY_PATH="/home/fayolle/workspace/llama.cpp/build/bin/llama-server"

# echo "=== √âtape 1/4 : V√©rification des d√©pendances syst√®me ==="
# OS="$(uname -s)"
# case "$OS" in
#     Linux*)
#         if [ -f /etc/debian_version ]; then
#             PACKAGES="libreoffice tesseract-ocr tesseract-ocr-fra wget poppler-utils"
#             for pkg in $PACKAGES; do
#                 if ! dpkg -l | grep -q "ii  $pkg "; then
#                     echo "Installation de $pkg..."
#                     sudo apt-get update && sudo apt-get install -y $pkg
#                 else
#                     echo "‚úì $pkg est d√©j√† install√©."
#                 fi
#             done
#         else
#             echo "AVERTISSEMENT : Ce script ne g√®re que Debian/Ubuntu. Veuillez installer manuellement les d√©pendances."
#         fi
#         ;;
#     Darwin*)
#         echo "macOS d√©tect√©. V√©rification avec Homebrew..."
#         if ! command -v brew &> /dev/null; then echo "Homebrew n'est pas install√©. Veuillez l'installer." >&2; exit 1; fi
#         PACKAGES="libreoffice tesseract tesseract-lang wget"
#         for pkg in $PACKAGES; do
#             if ! brew list --formula | grep -q "^$pkg$"; then
#                 echo "Installation de $pkg..." && brew install $pkg
#             else
#                 echo "‚úì $pkg est d√©j√† install√©."
#             fi
#         done
#         ;;
#     *)
#         echo "Syst√®me d'exploitation non support√© : $OS. Pour Windows, utilisez start.bat." >&2; exit 1;;
# esac

# echo -e "\n=== √âtape 2/4 : V√©rification du mod√®le LLM ==="
# mkdir -p "$MODEL_DIR"
# if [ ! -f "$MODEL_PATH" ]; then
#     echo "Mod√®le LLM non trouv√©. T√©l√©chargement en cours (~4.7 Go)..."
#     wget -O "$MODEL_PATH" "$MODEL_URL"
#     echo "‚úì Mod√®le t√©l√©charg√©."
# else
#     echo "‚úì Le mod√®le LLM est d√©j√† pr√©sent."
# fi

# echo -e "\n=== √âtape 3/4 : V√©rification de l'environnement Conda ==="
# # V√©rifie si l'environnement existe. Si non, le cr√©e.
# if ! conda env list | grep -q "$ENV_NAME"; then
#     echo "Cr√©ation de l'environnement Conda '$ENV_NAME'..."
#     conda env create -f environment.yml
#     echo "‚úì Environnement cr√©√©."
# else
#     echo "‚úì L'environnement Conda '$ENV_NAME' existe d√©j√†."
# fi

# echo -e "\n=== √âtape 4/4 : Lancement du serveur LLM ==="
# echo "Utilisation du mod√®le : $MODEL_PATH"
# echo "Pour arr√™ter le serveur, appuyez sur CTRL+C."

# echo "=================================================="
# echo " Lancement du serveur Llama.cpp"
# echo "=================================================="
# echo "Mod√®le : $MODEL_PATH"
# echo "URL    : http://127.0.0.1:8080"
# echo "Note   : Gardez ce terminal ouvert !"
# echo "=================================================="

# # Explication des flags :
# # -m : chemin du mod√®le
# # -c : taille du contexte (8192 est bien pour du RAG, augmente si n√©cessaire)
# # -ngl : nombre de couches sur le GPU (99 = max)
# # --host : adresse d'√©coute
# # --port : port d'√©coute
# # --n-predict : nombre max de tokens en r√©ponse (-1 = infini/limite contexte)

# $BINARY_PATH \
#     -m "$MODEL_PATH" \
#     -ngl 99 \
#     --host 127.0.0.1 \
#     --port 8080 \
#     --n-predict -1 \
#     --ctx-size 25000 \

#!/bin/bash
# Script de configuration ET de lancement pour le projet RAG (Linux/macOS)

# Arr√™te le script si une commande √©choue
set -e

# --- 1. Chargement de la configuration depuis .env ---
if [ -f .env ]; then
    # 'set -a' exporte automatiquement les variables vers les sous-processus
    set -a
    source .env
    set +a
    echo "‚úÖ Configuration charg√©e depuis .env"
else
    echo "‚ùå ERREUR : Fichier .env introuvable √† la racine."
    echo "   Veuillez copier .env_example vers .env et ajuster les chemins."
    exit 1
fi

# --- D√©finition des constantes compl√©mentaires ---
ENV_NAME="ragcdl"
# URL de t√©l√©chargement (On la garde ici car elle ne change pas souvent, 
# ou tu peux l'ajouter au .env si tu veux changer de mod√®le souvent)
MODEL_URL="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"


echo "=== √âtape 1/4 : V√©rification des d√©pendances syst√®me ==="
OS="$(uname -s)"
case "$OS" in
    Linux*)
        if [ -f /etc/debian_version ]; then
            # Ajout de 'curl' qui est souvent utile
            PACKAGES="libreoffice tesseract-ocr tesseract-ocr-fra wget poppler-utils curl"
            for pkg in $PACKAGES; do
                if ! dpkg -l | grep -q "ii  $pkg "; then
                    echo "Installation de $pkg..."
                    sudo apt-get update && sudo apt-get install -y $pkg
                else
                    echo "‚úì $pkg est d√©j√† install√©."
                fi
            done
        else
            echo "AVERTISSEMENT : Ce script ne g√®re que Debian/Ubuntu. Veuillez installer manuellement les d√©pendances."
        fi
        ;;
    Darwin*)
        echo "macOS d√©tect√©. V√©rification avec Homebrew..."
        if ! command -v brew &> /dev/null; then echo "Homebrew n'est pas install√©. Veuillez l'installer." >&2; exit 1; fi
        PACKAGES="libreoffice tesseract tesseract-lang wget curl"
        for pkg in $PACKAGES; do
            if ! brew list --formula | grep -q "^$pkg$"; then
                echo "Installation de $pkg..." && brew install $pkg
            else
                echo "‚úì $pkg est d√©j√† install√©."
            fi
        done
        ;;
    *)
        echo "Syst√®me d'exploitation non support√© : $OS." >&2; exit 1;;
esac

echo -e "\n=== √âtape 2/4 : V√©rification du mod√®le LLM ==="
# On utilise le chemin d√©fini dans le .env
MODEL_DIR=$(dirname "$LLM_MODEL_PATH")
mkdir -p "$MODEL_DIR"

if [ ! -f "$LLM_MODEL_PATH" ]; then
    echo "Mod√®le LLM non trouv√© √† : $LLM_MODEL_PATH"
    echo "T√©l√©chargement en cours (~4.7 Go)..."
    wget -O "$LLM_MODEL_PATH" "$MODEL_URL"
    echo "‚úì Mod√®le t√©l√©charg√©."
else
    echo "‚úì Le mod√®le LLM est d√©j√† pr√©sent : $LLM_MODEL_PATH"
fi

echo -e "\n=== √âtape 3/4 : V√©rification de l'environnement Conda ==="
# On v√©rifie si conda est disponible avant de lancer la commande
if command -v conda &> /dev/null; then
    if ! conda env list | grep -q "$ENV_NAME"; then
        echo "Cr√©ation de l'environnement Conda '$ENV_NAME'..."
        conda env create -f environment.yml
        echo "‚úì Environnement cr√©√©."
    else
        echo "‚úì L'environnement Conda '$ENV_NAME' existe d√©j√†."
    fi
else
    echo "‚ö†Ô∏è  Conda n'est pas d√©tect√©, on suppose que vous g√©rez votre venv manuellement."
fi

echo -e "\n=== √âtape 4/4 : Lancement du serveur LLM ==="


# On r√©cup√®re le dossier o√π se trouve l'ex√©cutable llama-server
BIN_DIR=$(dirname "$LLM_BINARY_PATH")
# On ajoute ce dossier √† la liste des endroits o√π Linux cherche les librairies (.so)
export LD_LIBRARY_PATH="$BIN_DIR:$LD_LIBRARY_PATH"



echo "=================================================="
echo "üöÄ Lancement du serveur Llama.cpp"
echo "=================================================="
echo "Ex√©cutable : $LLM_BINARY_PATH"
echo "Mod√®le     : $LLM_MODEL_PATH"
echo "Adresse    : http://$LLM_HOST:$LLM_PORT"
echo "Contexte   : $LLM_CONTEXT_SIZE tokens"
echo "Note       : Gardez ce terminal ouvert !"
echo "=================================================="

# V√©rification finale de l'ex√©cutable
if [ ! -f "$LLM_BINARY_PATH" ]; then
    echo "‚ùå ERREUR : L'ex√©cutable llama-server est introuvable √† l'adresse :"
    echo "   $LLM_BINARY_PATH"
    echo "   V√©rifiez la variable LLM_BINARY_PATH dans votre fichier .env"
    exit 1
fi

# Lancement avec les variables du .env
"$LLM_BINARY_PATH" \
    -m "$LLM_MODEL_PATH" \
    -ngl "$LLM_GPU_LAYERS" \
    --host "$LLM_HOST" \
    --port "$LLM_PORT" \
    --n-predict -1 \
    --ctx-size "$LLM_CONTEXT_SIZE"