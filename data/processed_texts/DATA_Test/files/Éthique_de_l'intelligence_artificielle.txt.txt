L'éthique de l'intelligence artificielle est le sous-domaine de l'éthique de l'informatique propre à l'intelligence artificielle (IA). Elle recouvre divers thèmes présentant des enjeux éthiques particuliers, dont les biais algorithmiques, l'équité, la prise de décision automatisée, la protection des données personnelles, l'imputabilité et la réglementation. L'éthique de l'IA recouvre également divers enjeux émergents ou potentiellement futurs tels que la sûreté et l'alignement de l'IA, la course aux armements, les systèmes d'armes létales autonomes, le chômage technologique, la désinformation, comment traiter certains systèmes d'IA s'ils ont un statut moral (bien-être et droits de l'IA), la superintelligence artificielle et les risques existentiels. Certains domaines d'application de l'IA peuvent aussi avoir des implications éthiques particulièrement importantes, comme la santé, l'éducation ou le militaire.

Préoccupations éthiques
Atteinte à la vie privée
L'émergence de l'intelligence artificielle (IA) soulève deux préoccupations majeures autour de la vie privée : des informations privées peuvent être compromises à travers une attaque contre un système d'IA par une tierce personne et un système d'IA lui-même peut être utilisé comme outil pour collecter des données privées sur des individus.
On trouve de nombreux exemples d'attaques contre un système d'IA par une tierce personne dans le domaine de l'apprentissage machine. En effet, les algorithmes d'apprentissage machine apprennent sur des données qui contiennent parfois des informations privées sur des individus, comme des informations médicales. L'entrainement de cet algorithme d'apprentissage machine produit ensuite un modèle qui peut être utilisé à travers des requêtes pour produire de nouvelles données. Il est donc nécessaire que les données d'entrainement ne puissent pas être reconstruites ou retrouvées à travers ces requêtes faites au modèle entrainé.
La recherche scientifique a permis de mettre en lumière plusieurs attaques permettant de reconstruire des données utilisées pour entrainer le modèle d'apprentissage machine à partir de requêtes au modèle. Par exemple, il est possible de récupérer des clés de chiffrement privées en utilisant des outils de complétion de code basés sur l'apprentissage machine comme GitHub Copilot. D'autres recherches ont démontré qu'il était possible de savoir si un individu fait partie d'une base de données d'entrainement sans y avoir directement accès. Cela peut porter atteinte à la vie privée de l'individu dans le cas où le simple fait de faire partie de la base de données est une information compromettante, par exemple lorsque la base de données regroupe des patients atteints d'une maladie grave.
De nombreuses technologies d'IA inquiètent les chercheurs quant à leur capacité à porter atteinte à la vie privée. Par exemple, les technologies de surveillance de masse comme la reconnaissance faciale peuvent représenter un danger pour la vie privée des individus.

Atteinte à la dignité humaine
Joseph Weizenbaum a soutenu en 1976 que la technologie de l'IA ne devrait pas être utilisée pour remplacer des personnes dans des positions qui exigent le respect et les soins, tels qu'un représentant du service à la clientèle (l'IA est déjà utilisée pour les systèmes téléphoniques interactifs de réponse vocale), un thérapeute (comme le proposait sérieusement Kenneth Colby dans les années 1970), une assistance aux personnes âgées (comme l'a rapporté Pamela McCorduck dans son livre The Fifth Generation[réf. nécessaire]), un soldat, un juge ou un policier. Weizenbaum explique que nous avons besoin de sentiments et d'empathie de la part des personnes occupant ces postes. Si les machines les remplacent, nous nous retrouverons aliénés, dévalués et frustrés. L'intelligence artificielle, si elle est utilisée de cette façon, représente une menace pour la dignité humaine.
Le fondateur d'IA John McCarthy s'oppose au ton moralisateur de la critique de Weizenbaum. « Quand la moralisation est à la fois véhémente et vague, elle invite l'abus autoritaire », écrit-il.
Bill Hibbard (en) écrit que « la dignité humaine exige que nous nous efforcions d'éliminer notre ignorance de la nature de l'existence, et l'IA est nécessaire pour cet effort ».

Transparence et explicabilité
La transparence de l'IA est la capacité pour ses utilisateurs de comprendre comment les décisions sont prises par les algorithmes. Les décisions prises par l'IA peuvent avoir un impact significatif sur les humains, il est donc crucial que les intelligences artificielles soient transparentes en ce qui concerne leurs décisions, de façon à permettre d'identifier leurs biais potentiels. Cela pourra également permettre de renforcer la confiance envers les systèmes d'IA et de favoriser l'adoption de ces technologies dans le futur selon le besoin.
Quelques méthodes permettent de rendre les systèmes d'IA plus transparents. L'une des approches consiste à utiliser des techniques d'exploration de modèle pour comprendre comment l'IA prend ses décisions. Cela peut être fait en examinant les poids et les biais des algorithmes et en les comparant à des ensembles de données d'entraînement[réf. nécessaire].
Une autre méthode consiste à rendre les systèmes d'IA plus interprétables. Des techniques telles que la représentation graphique des résultats permettent ainsi de visualiser comment les données sont traitées par l'IA. Ces méthodes ne permettent toutefois pas de comprendre réellement les décisions prises par les intelligences artificielles, donc ne permettent pas de les prédire.
La transparence de l'IA est également nécessaire pour s’assurer que les systèmes d'IA soient équitables et non discriminatoires.

Ethics washing
Dans le domaine de l’intelligence artificielle, l’effervescence des principes éthiques questionne et suggère une potentielle notion d'ethics washing de la part des entreprises. Cette expression fait référence au phénomène qui ferait de l’éthique un argument de vente, un phénomène de mode. En ce sens, les « ethics guidelines » des entreprises interrogent sur ce qui est mis derrière la notion d'éthique, et si ces lignes directrices sont vraiment utiles, ou alors davantage un moyen de casser la controverse.

open source et openness
La notion d’openness est utilisée dans le domaine de l'IA pour généraliser la notion d’open source à tous les composants d'un système d'IA. Alors que l’open source fait uniquement référence au code source d'un système, d'autres composants comme les poids peuvent également être accessibles et redistribués librement. Le terme d'IA open source est cependant parfois utilisé par abus de langage par la communauté scientifique pour désigner une IA dont la plupart de ses composants sont libres d'accès. L'openness impacte la transparence, la démocratisation, l'auditabilité et la sécurité d'un système d'IA.
Bill Hibbard soutient que parce que l'IA aura un effet profond sur l'humanité, les développeurs d'IA sont des représentants de l'humanité future et ont donc l'obligation éthique d'être transparents dans leurs travaux. Ben Goertzel et David Hart ont créé OpenCog, un cadre open source pour le développement de l'IA. OpenAI est une société de recherche à but non lucratif créée par Elon Musk, Sam Altman et d'autres pour développer l'IA open source. De nombreux autres modèles de fondation open ont vu jour comme LLaMA et Stable Diffusion.
La démocratisation de l'IA à travers l’open source et l’openness a causé l'apparition de nouveaux dangers et d'utilisations détournées de modèles d'Apprentissage automatique. Par exemple, l'accès facilité à des modèles d'apprentissage dans le domaine de la Vision par ordinateur a permis à des utilisateurs de Reddit de créer des deepfakes pornographiques. Cet accroissement des risques liés à l'utilisation malveillante de systèmes d'Intelligence Artificielle a mené la communauté scientifique à remettre en question le bien-fondé de l’open source et de l’openness dans le domaine de l'IA. La puissance de calcul nécessaire pour affiner l'entrainement d'un algorithme d'IA étant de plus en plus accessible à bas coût, le libre accès aux poids d'un algorithme d'apprentissage peut donc faciliter l'apparition d'utilisation malveillante.
Des questionnements similaires sont soulevés sur le libre accès des publications scientifiques en intelligence artificielle.

Alignement et éthique des machines
L'éthique des machines (ou morale de la machine) est le domaine de recherche qui se consacre à la conception d'agents moraux artificiels (AMA), de robots ou d'ordinateurs artificiellement intelligents qui se comportent moralement.
Isaac Asimov a examiné cette question dans les années 1950 dans Les Robots. À l'insistance de son rédacteur John W. Campbell Jr., celui-ci a proposé les trois lois de la robotique dans le but de gouverner des systèmes artificiellement intelligents. Une grande partie de son travail a ensuite été consacrée à tester les limites de ses trois lois. Son travail suggère qu'aucun ensemble de lois fixes ne peut anticiper suffisamment toutes les circonstances possibles.
Certains experts et universitaires ont mis en doute l'utilisation de robots pour le combat militaire, surtout lorsque ces robots sont dotés d'un certain degré d'autonomie. Le président de l'Association for the Advancement of Artificial Intelligence a commandé une étude afin d'examiner cette question. Ils pointent vers des programmes d'Acquisition de Langage afin d'imiter l'interaction humaine.
Vernor Vinge a suggéré un point hypothétique de la civilisation humaine qui pourrait connaître une croissance technologique où certains ordinateurs deviendraient plus intelligents que les humains. Il appelle cela la « Singularité ». Il suggère que celle-ci peut être très dangereuse pour l'humanité. Cela est discuté par le singularitarisme. La Machine Intelligence Research Institute a suggéré la nécessité de construire des « intelligences artificielles amicales », ce qui signifie que les progrès qui sont déjà en cours avec l'IA devrait également inclure un effort pour rendre l'IA intrinsèquement amicale et humaine.
Dans Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach et Colin Allen concluent que les tentatives d'enseigner les droits des robots à partir du mal vont probablement améliorer la compréhension de l'éthique humaine en motivant les humains à combler les lacunes de la théorie normative moderne et en fournissant une plate-forme pour l'investigation expérimentale. Nick Bostrom et Eliezer Yudkowsky ont plaidé pour des arbres de décision (tels que ID3) sur des réseaux de neurones et des algorithmes génétiques au motif que les arbres de décision obéissent aux normes sociales modernes de transparence et de prévisibilité.

Roboéthique
La « roboéthique » envisage l'aspect éthique de la conception, la construction, l'utilisation, et le traitement des robots par les humains. Elle considère à la fois comment des êtres artificiellement intelligents peuvent être utilisés pour nuire à l'homme et comment elles peuvent être utilisées au profit de celui-ci.

Violences sexuelles et pédopornographie
Les modèles d'IA génératifs sont maintenant capables de générer des images photo-réalistes, permettant notamment la génération de contenu pédopornographique. En octobre 2023, l'Internet Watch Foundation (IWF) sonne l'alarme dans un rapport explicitant l'étendue du problème : près de 3 000 images pédopornographiques générées par IA ont été retrouvées sur le web et jugées illégales dans le cadre de la loi britannique. Certaines de ces images utilisent le visage de vrais enfants, étant générées à partir d'autres photos données en entrée au modèle d'IA. Le rapport de L'IWF s'inquiète notamment de ce type de modèle d'IA lorsqu'ils sont open source  car cette génération d'images peut être effectuée en local sur un ordinateur, rendant la détection de ce type de pratique plus difficile.
Un autre type de violence sexuelle causée par l'IA est la génération de deepfakes pornographiques. Les deepfakes postées en ligne sont pour 96 % à caractère pornographique et sont généralement générées sans le consentement des personnes dont l'image est utilisée, touchant principalement des femmes célèbres.

Droits des robots
Les droits des robots sont les obligations morales de la société vis-à-vis de ses machines, similairement aux droits de l'homme et aux droits des animaux. Il peut s'agir du droit à la vie et à la liberté, à la liberté de pensée et d'expression et à l'égalité devant la loi.

Superintelligence et imprévisibilité
De nombreux chercheurs ont fait valoir que, dans le cadre d'une « explosion d'intelligence » au cours du XXIe siècle, une IA auto-améliorée pourrait devenir tellement puissante que les humains ne pourraient pas l'empêcher d'atteindre ses objectifs.
Dans son article Ethical Issues in Advanced Artificial Intelligence et son livre Superintelligence : chemins, dangers, stratégies, le philosophe Nick Bostrom met en garde contre les risques existentiels liés à l'IA. Selon lui, une superintelligence artificielle, dotée de capacités de planification et d'initiative, pourrait potentiellement causer l'extinction de l'humanité. De telles IA pourraient en théorie avoir des motivations radicalement différentes de celles des humains, et ce serait aux concepteurs de leur inculquer des valeurs morales. Bostrom soutient qu'une superintelligence artificielle serait très compétente pour déjouer tout ce qui pourrait entraver la réalisation de ses objectifs. Il suggère que la première superintelligence artificielle créée pourrait être décisive pour l'humanité et invite à davantage de recherche sur les aspects de sûreté.
Bostrom considère qu'une superintelligence artificielle aurait également le potentiel de résoudre de nombreux problèmes difficiles, tels que la maladie, la pauvreté et la dégradation de l'environnement, et pourrait aider l'humanité à augmenter ses capacités.
Bill Hibbard propose une conception de l'IA[Laquelle ?] qui évite plusieurs types de comportement non intentionnel de celle-ci, y compris l'auto-désillusion, les actions instrumentales non intentionnelles et la corruption du générateur de récompense.
Dans son film Singularity (2017) Robert Kouba imagine qu'un super ordinateur (Kronos) décrète que l’homme est la plus grande menace pour la planète, et cherche à éradiquer l'espèce humaine.

Technologies controversées
Armes autonomes
Certains experts et universitaires ont mis en doute l'utilisation de robots pour le combat militaire, surtout lorsque ces robots sont dotés d'un certain degré d'autonomie. La Marine américaine a financé un rapport qui indique que, à mesure que les robots militaires deviennent plus complexes, il faudrait accorder plus d'attention aux implications de leur autonomie. Des chercheurs affirment que, si bien conçus, les robots autonomes pourraient être plus éthiques que les humains dans leurs prises de décision, et s'avérer bénéfiques dans la guerre.
Les armes autonomes avec IA peuvent présenter un type de danger différent de celui des armes commandées par des humains. De nombreux gouvernements ont commencé à financer des programmes pour développer l'armement à base d'intelligence artificielle. La marine des États-Unis a annoncé des plans de développement des drones de combat autonomes, en parallèle des annonces similaires de la Russie et de la Corée.
En ce qui concerne la possibilité d'utiliser militairement des systèmes plus intelligents, Open Philanthropy écrit que ce scénario « semble potentiellement être aussi important que les risques liés à la perte de contrôle », mais que les organismes de recherche qui enquêtent sur l'impact social à long terme des IA ont consacré relativement peu de temps à cette préoccupation : « cette classe de scénarios n'a pas été un axe majeur pour les organisations les plus actives dans ce domaine, comme la Machine Intelligence Research Institute (MIRI) et Future of Humanity Institute (FHI), il semble y avoir eu moins d'analyses et de débats à leur sujet ».

Lettres ouvertes de la communauté de l'IA
« Priorités de recherche pour une intelligence artificielle robuste et bénéfique » (2015)
Stephen Hawking, Elon Musk, Max Tegmark et plusieurs autres de la communauté internationale en intelligence artificielle et en robotique ont signé une Lettre ouverte sur l'intelligence artificielle du Future of Life Institute (« Institut de l'avenir de la vie »). Cette lettre fait référence à un article publié séparément sur les priorités en recherche sur l'IA.

« Armes autonomes : une lettre ouverte de chercheurs en IA et en robotique » (2015)
Une lettre ouverte sur les armes autonomes est publiée :
« Si une puissance militaire majeure avance avec le développement de l'arme de l'intelligence artificielle, une course aux armements mondiale est pratiquement inévitable, et le point final de cette trajectoire technologique est évident : les armes autonomes deviendront les Kalachnikov de demain », dit la pétition, qui inclut le cofondateur de Skype, Jaan Tallinn et professeur de linguistique du MIT Noam Chomsky en tant que partisans supplémentaires contre l'armement des IA.
Le message porté par Hawking et Tegmark indique que les armes à base d'IA représentent un danger immédiat et que des mesures sont nécessaires pour éviter une catastrophe dans un avenir proche.

« Lettre ouverte à la Convention des Nations unies sur certaines armes classiques » (2017)
Une autre lettre ouverte au sujet de la Convention sur certaines armes classiques des Nations unies cible spécifiquement le bannissement des armes autonomes dans le cadre des Nations unies. Cette lettre, « AN OPEN LETTER TO THE UNITED NATIONS CONVENTION ON CERTAIN CONVENTIONAL WEAPONS » , est signée par Elon Musk de Tesla et par Mustafa Suleyman d'Alphabet ainsi que par 116 spécialistes de 26 pays.

« Les armes létales autonomes menacent de devenir la troisième révolution en armement. Une fois développées, ces armes permettront de combattre dans un conflit armé à une échelle plus grande que jamais, et à des échelles de temps plus rapides que les humains ne peuvent le comprendre. Elles peuvent être des armes de terreur, des armes que des despotes et des terroristes utilisent contre des populations innocentes, et des armes piratées pour se comporter de manière indésirable. Nous n'avons pas longtemps pour agir. Une fois cette boîte de Pandore ouverte, il sera difficile de la fermer. Nous implorons donc les Hautes Parties contractantes de trouver un moyen de nous protéger tous de ces dangers. »

Le chercheur canadien Yoshua Bengio a écrit au premier ministre Trudeau pour demander, avec ses collègues, « que le Canada appuie les démarches en cours à l’ONU afin de bannir les robots tueurs. C’est pour cela que j’ai contribué, aux côtés de Stephen Hawkins et Elon Musk, à l’élaboration d’une autre lettre, cette fois-ci provenant de toute la communauté internationale en intelligence artificielle et en robotique, et exigeant un traité international pour bannir les robots tueurs. »

Positions inflationniste et déflationniste
Abordé par le philosophe Nick Bostrom dans son ouvrage intitulé Superintelligence : chemins, dangers, stratégies, le « risque existentiel » engendré par l'intelligence artificielle est abordé à l'aide de deux positions éthiques : l'inflationnisme et le déflationnisme. Selon Bostrom, l'émergence d'une intelligence bien plus importante que l'intelligence humaine serait à venir et ainsi, la divergence de points de vue opposerait ceux étant pour le développement à long terme de cette nouvelle technologie et ceux pensant plutôt aux enjeux actuels associés à son implantation dans notre société.

Point de vue inflationniste
« La perspective inflationniste met l’accent sur les conséquences à long terme du développement de l’IA en réfléchissant notamment aux enjeux et aux risques associés à l’émergence d’une IA générale (IAG). » Ainsi, les gens adhérant à ce point de vue désirent créer des machines considérées éthiques selon les valeurs présentes dans la société moderne. Ils partagent l'idée qu'un jour l'intelligence des machines surpassera celle des humains et qu'une IA pourra « exercer la plupart des professions humaines au moins aussi bien qu’un être humain moyen ». Par exemple, l’astrophysicien Stephen Hawking a déclaré :

« [j]e pense que le développement d’une intelligence artificielle complète pourrait mettre fin à l’humanité. Une fois que les humains auraient développé l’IA, celle-ci décollerait seule, et se redéfinirait de plus en plus vite. Les humains, limités par une lente évolution biologique, ne pourraient pas rivaliser et seraient dépassés. »

En janvier 2015, la lettre ouverte sur l'intelligence artificielle, défendue par des dizaines d'experts en intelligence artificielle et des personnalités comme Stephen Hawking et Elon Musk, semble concorder avec la position inflationniste en déclarant que bien que l'intelligence puisse éradiquer diverses maladies et la pauvreté, il est nécessaire de superviser son progrès de près pour éviter une perte de contrôle. Intitulé Priorités de recherches pour une intelligence artificielle solide et bénéfique : Lettre ouverte (en version originale en anglais Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter), ce document vise à mettre en place des priorités pour les enjeux économiques, éthiques et sociaux, entre autres. Par exemple, « [u]n autre [enjeu] fréquemment évoqué est celui de systèmes d’IA fortes capables de programmer de façon autonome des algorithmes d’IA toujours plus performants, créant ainsi une distance supplémentaire entre l’IA et l’être humain ».
Jean-Gabriel Ganascia a critiqué les aspects futuristes et spéculatifs des discours sur l’éthique de l’IA tenus par certaines personnalités comme Elon Musk, Bill Gates ou encore Stephen Hawking.

Point de vue déflationniste
La perspective déflationniste « cherche plutôt à penser les enjeux éthiques spécifiques qui sont actuellement associés au développement des systèmes d’IA réels et à leur implantation dans nos sociétés ». En se concentrant sur les enjeux liés à l'intelligence artificielle, les partisans se ralliant à la philosophie déflationniste priorisent les réglementations visant à limiter et réduire les possibles dégâts causés par une utilisation inadéquate de cette nouvelle technologie. Dans cette même optique de création de normes, le Forum IA responsable, appuyé par l'institut québécois de l'intelligence artificielle Mila, a proposé une première version de la Déclaration de Montréal pour un développement responsable de l'intelligence artificielle. Celle-ci appuie la position déflationniste afin de prioriser le développement de l’IA dans l’intérêt de l’humanité : « [l]e document déclare que l’IA doit ultimement promouvoir le bien-être de tous les êtres sensibles, la participation éclairée à la vie publique et les débats démocratiques ». Suivant cette perspective, plusieurs autres grandes organisations ont rédigé des documents visant à réguler l'utilisation de l'intelligence artificielle. Un exemple de réglementations mises en place pour favoriser son usage sont les normes ISO. Selon l'Organisation internationale de normalisation, ces « document[s], établi[s] par consensus et approuvé[s] par un organisme reconnu » sont nécessaires afin de démystifier les effets potentiels de l'IA. L'objectif visant à la « normalisation » de l'IA  s'inscrit dans une perspective déflationniste afin que l'intelligence artificielle tienne compte de plusieurs enjeux éthiques tels que l'espionnage informatique et les biais algorithmiques :
« La future norme ISO/IEC CD 23894, quant à elle, portera sur la gestion des risques des systèmes d’intelligence artificielle. Ce projet de norme s’appuie sur la norme ISO 31000:2018 - Management du risque, qui permet l’analyse et l’évaluation des risques, tels que les atteintes à la réputation ou à la marque, la cybercriminalité ou encore les risques politiques et le terrorisme[réf. nécessaire]. »
Une autre proposition de réglementation a été publiée en Europe le 21 avril 2021, dite Législation sur l'intelligence artificielle (Artificial Intelligence Act), et vise à rendre les appareils « digne[s] de confiance, centré[s] sur l’humain, éthique[s], durable[s] et inclusi[fs] », appuyant ainsi le point de vue déflationniste.

Législations, principes, directives et guides d'éthiques
Non-uniformisation des principes éthiques
Anna Jobin, Marcello Ienca et Effy Vayena ont identifié 11 principes éthiques : transparence, justice/équité, non-malfaisance, responsabilité, respect de la vie privée, bienfaisance, liberté et autonomie, confiance, durabilité, dignité humaine et solidarité. Ces principes sont ici rangés du plus représenté au moins représenté. Les 84 documents qu’ils ont utilisés sont issus de différents types d’institutions : du secteur privé, du secteur public, des institutions académiques et de recherche, des associations professionnelles et sociétés scientifiques, des organisations sans but lucratif, etc.
Les principes ne figurent pas tous de manière systématique dans les guides éthiques, l’éthique de l’intelligence artificielle n’est donc pas un domaine unifié. Comme l’expliquent les auteurs, la notion d’éthique dans le domaine de l’IA est une notion floue qui peut recouvrir plusieurs réalités. On ne retrouve aucun principe éthique commun à l’ensemble de ces textes. Certains d'entre eux se démarquent tout de même tandis que d’autres se retrouvent sous-représentés. La question de la transparence, par exemple, même si elle n’apparaît pas dans tous les documents, dessine une forme de convergence. À l’opposé, les principes tels que la dignité (humaine) et la durabilité (sustainability) se retrouvent sous-représentés.
Cette notion de durabilité renvoie la plupart du temps à l’empreinte écologique de ces technologies, qui sont très gourmandes en données et donc possèdent une forte empreinte écologique. En dehors de cela, les auteurs soulignent que ce principe de sustainability est intimement lié à celui de solidarité. En ce sens, en dehors des impacts négatifs qu’a l’intelligence artificielle sur l’environnement, il s’agit de créer une technologie qui puisse perdurer, avec des données traitées de façon durable et dont les enseignements sont exploitables dans le temps. L’intelligence artificielle pourrait être conçue, développée et régie dans le but d’augmenter son efficacité énergétique tout en limitant son empreinte écologique.
Le principe de solidarité est, dans la plupart des cas, évoqué en référence à la perte des emplois et à la substitution de l’homme par la machine. Les auteurs soulignent qu’il est nécessaire de questionner ce principe en regard de la question humanitaire, pour aider à résoudre les défis humanitaires notamment.
Des solutions pour rendre l'intelligence artificielle plus éthique sont évoquées dans ces guides. L’ethics by design ou éthique a priori, par exemple, fait référence à l’idée d’une éthique qui ne se concentrerait non plus sur l’aval de la conception de l’IA mais bien sur l’amont de celle-ci. Depuis plusieurs années, les questions d’éthique de l’intelligence artificielle retentissent de plus en plus, dans les médias, aussi bien que dans toutes les sphères impliquées, ce qui a poussé le retrait de certains produits, applications ou algorithmes du marché du numérique. La solution serait donc de penser en amont de la conception, les éventuelles dérives de ces technologies afin d’en limiter les conséquences néfastes. Des solutions techniques sont apportées en ce sens, par exemple, on tente de mettre en place des techniques pour « débiaiser » l'IA.
En dehors de la technologie, on retrouve dans deux documents l’importance de mieux protéger les lanceurs d’alerte, ici, nous faisons référence à la notion forgée par Francis Chateauraynaud en 1996. Cette protection pourrait permettre de libérer la parole afin d'éveiller les consciences sur les éventuelles dérives de cette technologie.

Principes d’Asilomar
Plusieurs milliers de signataires, dont Stephen Hawking et Elon Musk ont « adopté un guide de référence pour un développement éthique de l'intelligence artificielle ». Ces principes furent élaborés durant la conférence Asilomar de 2017.

Déclaration de Montréal pour un développement responsable de l'intelligence artificielle
Cette déclaration est une initiative lancée par l'Université de Montréal avec le soutien de l'institut québécois de l'intelligence artificielle Mila dans le but de poser des principes éthiques et responsables au développement de l'intelligence artificielle, qui a abouti à une présentation publique le 4 décembre 2018 d'une première version officielle.
La déclaration énonce dix principes à respecter : le bien-être, le respect de l'autonomie, la protection de l'intimité et de la vie privée, la solidarité, la participation démocratique, l'équité, l’'nclusion de la diversité, la prudence, la responsabilité et le développement soutenable.

Recommandation sur l'éthique de l'intelligence artificielle de l'UNESCO
Le 23 novembre 2021, les 193 États membres de l'Organisation des Nations Unies pour l'éducation, la science et la culture (UNESCO) ont adopté à l'unanimité un cadre normatif mondial, qui prévoit les garde-fous éthiques nécessaires. Ce cadre, constitué par une recommandation sur l'éthique de l'intelligence artificielle, a vocation à être transposé sous la forme de stratégies et de réglementations nationales.
La recommandation comprend quatre valeurs

respect, protection et promotion des droits de l’homme, des libertés fondamentales et de la dignité humaine ;
un environnement et des écosystèmes qui prospèrent ;
assurer la diversité et l'inclusion ;
vivre dans des sociétés pacifiques, justes et interdépendantes.
La recommandation comprend dix principes :

principes de proportionnalité et d'innocuité ;
sûreté et sécurité ;
équité et non-discrimination ;
durabilité ;
droit au respect de la vie privée et protection des données ;
surveillance et décision humaine ;
transparence et explicabilité ;
responsabilité et redevabilité ;
sensibilisation et éducation ;
gouvernance et collaboration multipartites et adaptatives.
La recommandation identifie 11 domaines d'action stratégiques.

L'IA digne de confiance et l'Europe
Face à des usages de l'intelligence artificielle parfois décriés, tels que les applications de surveillance décernant un score aux citoyens chinois, l'Europe souhaite se positionner comme un garant de l'éthique de l'intelligence artificielle. Elle constitue le 14 juin 2018 un comité consultatif de 52 experts indépendants, le Groupe d'experts indépendants de haut niveau sur l'intelligence artificielle (GEHN IA, ou High Level Group on Artificial Intelligence, AI HLEG), dont le rôle est de rédiger des documents à consultation ouverte sur l'IA digne de confiance, notamment des lignes directrices et des auto-évaluations.
Le 21 avril 2021, l'Europe vise à se doter d'une législation en matière d'IA. Elle propose en cela une ébauche de loi sur l'intelligence artificielle, l'IA Act, qui a été finalisée en 2023 et qui doit entrer en vigueur en 2025. Cette loi sur l'intelligence artificielle précise notamment des pratiques interdites en matière d'IA ainsi qu'une catégorisation de systèmes d'IA à haut risques : les IA qui sont intégrées en tant que composant de sécurité dans des systèmes déjà soumis à législation, et les IA autonomes avec des implications principalement dans les droits fondamentaux (voir par exemple la première ligne de ce chapitre).
En France, la CNIL se dote en janvier 2023 d'un service de l’intelligence artificielle (SIA), chargé de la « prévention des risques pour la vie privée liés à la mise en œuvre de ces systèmes » afin de « préparer l’entrée en application du règlement européen sur l’IA ».

Éthique de l'IA dans la littérature
Le Premier Cercle, publié par Alexandre Soljenitsyne en 1968, décrit l'utilisation de la compréhension automatique des langues et de la reconnaissance vocale au service de la tyrannie. Si un programme d'IA peut comprendre les textes et les discussions, alors, avec une puissance de traitement adéquate, il pourrait théoriquement écouter tous les courriels et les conversations téléphoniques du monde, ainsi que les comprendre et rendre compte des propos tenus par chacun. Un programme d'IA comme celui-ci pourrait permettre aux gouvernements et à d'autres entités de supprimer efficacement la dissidence et d'attaquer leurs ennemis.

Voir aussi
Articles connexes
Chercheurs

Organisations

Bibliographie
(en) Pamela McCorduck, Machines Who Think : A Personal Inquiry into the History and Prospects of Artificial Intelligence, 2004, 2e éd., 565 p. (ISBN 1-56881-205-1)

Liens externes
(fr) Recommandation sur l'éthique de l'intelligence artificielle, UNESCO, 23 novembre 2021
(fr) Yoshua Bengio, « La communauté de l’intelligence artificielle a bien fait ses devoirs », Le Devoir,‎ 5 décembre 2017 (lire en ligne)
(en) Robotics: Ethics of artificial intelligence. "Four leading researchers share their concerns and solutions for reducing societal risks from intelligent machines." Nature, 521, 415–418 (28 May 2015) doi:10.1038/521415a
(en) Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture

Références
(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Ethics of artificial intelligence » (voir la liste des auteurs).

 Portail de l’intelligence artificielle   Portail de la philosophie   Portail de la robotique   Portail de l’informatique