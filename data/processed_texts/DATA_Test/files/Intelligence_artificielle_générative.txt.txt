L'intelligence artificielle générative ou IA générative (IAg ou GenAI) est un type de système d'intelligence artificielle (IA) capable de générer du texte, des images, des vidéos ou d'autres médias en réponse à des requêtes (aussi appelées invites, ou en anglais prompts).
Elle semble avoir des applications possibles dans presque tous les domaines, avec une balance des risques et des opportunités encore discutée : l'IA générative est en effet aussi source d'inquiétudes et de défis éthiques, techniques et socioéconomiques à la hauteur des espoirs qu'elle suscite. Elle peut contribuer à des usages abusifs, accidentels ou détournés (militaires notamment), à une suppression massive d'emploi, à une manipulation de la population via la création de fausses nouvelles, de deepfakes ou de nudges numériques.
L'IA générative questionne aussi philosophiquement la nature de la conscience, de la créativité, de la paternité, et crée de nouvelles interactions homme-machine. Elle est encore (en 2024) peu régulée et la difficulté d’évaluer la qualité et la fiabilité des contenus générés ou l’impact sur la créativité et la propriété intellectuelle humaines est croissante. Certains experts craignent que des IA génératives à venir soient capables de manipuler les humains, d'accéder à des systèmes d'armes, d'exploiter des failles de cybersécurité, voire peut-être bientôt d'acquérir une forme de conscience ou de devenir incontrôlable au point de menacer l'existence de l'humanité. Un premier sommet mondial en sûreté de l'IA a été organisé en novembre 2023 à Bletchey Park au Royaume-Uni, où la création d'un équivalent du GIEC pour informer sur les risques liés à l'IA a notamment été discutée.

Vocabulaire
La littérature spécialisée anglophone parle aussi d'AIGC, l'acronyme de Artificial Intelligence Generated Content (« Contenu Généré par Intelligence Artificielle »).
L'IA générative est associée à des « modèles génératifs », qui produisent de multiples sorties (outputs ou résultats, qui vont du texte et de l'image à des agents conversationnels ou à l'impression 3D). Un modèle génératif peut être utilisé comme « fondation » d’autres systèmes. Et plusieurs modèles génératifs peuvent être associés. Par exemple avec ChatGPT, où le générateur de texte GPT-4 peut envoyer des requêtes à DALL-E 3 pour générer des images.
Une IA est dite multimodale quand elle peut analyser ou générer différents types de données. Par exemple, l'IA générative Google Gemini est multimodale, car bien que ne générant que du texte, elle peut analyser du texte, des images, du son, et des vidéos.
L'IA générative ne doit pas être confondue avec l'IA générale, qui est souvent définie comme une IA au moins aussi compétente que l'humain dans pratiquement toutes les tâches cognitives.
Un modèle de fondation, tel que défini par l’université de Stanford, est un modèle large (de grande taille) fondé sur une architecture de réseau de neurones profond, et entraîné sur une quantité massive de données non annotées (généralement par apprentissage auto-supervisé). Les grands modèles de langage (LLM pour Large Language Model) en font partie (entraînés sur un grand corpus de textes). Leur pré-entraînement est long et couteux, mais ils peuvent ensuite, moyennant quelques réglages, être adaptés et optimisés pour de nouvelles tâches, sans nécessiter beaucoup de données supplémentaires.
Le 28 octobre 2024, l'Open source initiative propose une première version de définition d'intelligence artificielle générative open-source, inspirée des 4 libertés de la définition du logiciel libre par la Free Software Foundation. Celle-ci impose une description complète des données d'entraînement sans pour autant imposer de mécanisme juridique pour décrire ces données. Cette définition exclut notamment Llama, l'IA générative de l'entreprise Meta, anciennement Facebook.

Capacités
Selon une analyse d’OpenAI en 2018 : « depuis 2012, la quantité de calcul utilisée dans les plus grands entraînements d’IA a augmenté de manière exponentielle avec un temps de doublement de 3,4 mois (en comparaison, la loi de Moore avait une période de doublement de deux ans).
Depuis 2012, cette métrique a augmenté de plus de 300 000 fois (une période de doublement de deux ans ne donnerait qu’une augmentation de sept fois). » Cette augmentation de la puissance de calcul a facilité l’émergence d’IA pouvant générer du texte, du code informatique, de la musique, des voix, des images, de la vidéo, des modèles 3D ou encore des séquences de mouvements.
Ces systèmes d'IA s'inspirent des données d'entraînement pour créer du contenu inédit. Ils ne se contentent pas de classer les données d'entrée qu'on leur a fournies, ni de prédire des données statistiquement probables. Ils génèrent des contenus nouveaux qui ne sont en général que partiellement similaires aux données sur lesquelles ils ont été entraînés. Divers filtres ou curseurs permettent de donner une plus ou moins grande liberté à l'IA. Certains modèles d'IA permettent d'ajouter aux invites textuelles des "prompts négatifs" (qui sont des invites expliquant ce qu'on ne veut pas voir dans la réponse attendue de l'IA).
Les versions publiques de grands modèles d'IA générative disponibles en 2022 et 2023 produisent des contenus modulés et filtrés de manière à limiter leurs biais, les contenus trompeurs, dangereux, racistes, biaisés, choquants, haineux, non sollicités, les images pornographiques ou sexuellement explicites. Au début des années 2020, la puissance de calcul de l’IA a doublé tous les six à dix mois, permettant aux modèles d’IA de progresser à un rythme exponentiel.
Les anglophones parlent de « Frontier AI » pour désigner les modèles d'IA aux capacités les plus élevées et générales, et qui pourraient présenter des risques nouveaux. Ce type d'IA s'est faite connaître du public par ChatGPT (et sa variante Bing Chat), un chatbot (agent conversationnel programmable) conçu par OpenAI à partir de ses grands modèles de langage de fondation GPT-3 et GPT-4, ainsi que par Bard, un chatbot de Google basé sur LaMDA, qui a évolué vers Gemini. D'autres modèles d'IA générative incluent des systèmes artistiques d'intelligence artificielle tels que Stable Diffusion, Midjourney et DALL-E. Ces IA ont un très large spectre d'applications potentielles dans des domaines créatifs (arts plastiques, cinéma, musique, écriture, design, météo, architecture...), mais aussi dans les secteurs de la santé, de la finance, des jeux vidéo et des simulateurs, dans tous les domaines des sciences et techniques, des sciences sociales, de l'industrie et de la connaissance. Elles ont récemment permis un bond en avant en biologie moléculaire et en compréhension de phénomènes physiques complexes. Elles permettent de synthétiser des visages et des voix humaines réalistes. Elles offrent de nouveaux modes d'exploration d'hypothèses et de scenarios (notamment depuis peu grâce à la production de données synthétiques sophistiquées, issues du domaine de la recherche générative assistée par IA)[réf. nécessaire].

Technologies sous-jacentes
Les cadres technologiques et conceptuels les plus importants pour aborder l'IA générative sont en cours d’élaboration depuis un certain temps, mais ils n'ont vraiment abouti que dans les années 2020 à plusieurs types de modèles d'IA particulièrement efficaces :

les réseaux antagonistes génératifs (GAN), composés de deux parties : un réseau générateur créant de nouveaux échantillons de données, et un réseau discriminateur qui évalue si les échantillons sont réels ou faux. Les deux réseaux sont entraînés ensemble dans le cadre d'un processus concurrentiel, le réseau générateur essayant continuellement de produire des échantillons de meilleure qualité et plus réalistes, tandis que le réseau discriminateur s'efforce d'identifier avec précision les faux échantillons ;
les auto-encodeurs variationnels (VAE) ;
les modèles de diffusion (en) (ex. : DALL-E, Stable Diffusion) ;
les transformeurs génératifs pré-entraînés (Generative Pretrained Transformers, ou GPT en anglais), un type de modèle d'apprentissage profond basé sur l'architecture transformeur, pré-entraîné sur de grands ensembles de données non étiquetées, équipé d'un « mécanisme d'attention » et capable de générer du nouveau texte semblable à celui écrit par des humains.
L'IA générative est encore loin de répondre « de manière fiable ou digne de confiance, et il reste encore beaucoup de travail à faire pour rendre ces sources fiables et impartiales », mais elle a des applications (actuelles ou potentielles) dans des domaines aussi variés que l’art, l'industrie, le jeu vidéo, la musique, la médecine, le développement logiciel, le marketing, les biotechnologies, la finance ou encore la mode.

Modalités
Une IA générative est souvent entraînée par apprentissage auto-supervisé sur de vastes ensembles de données. Ses capacités dépendent des types de données (modalité) utilisés pour l'entraînement, ce qui peut inclure du texte, des images fixes, du son ou des vidéos.
Une IA générative est dite « unimodale » quand elle ne peut accepter et créer qu'un seul type de données, comme du texte. Elle est appelée « multimodale » quand elle peut traiter ou générer plusieurs types de données, par exemple du texte et des images.
Les modalités possibles incluent :

du texte : les systèmes d'IA générative peuvent être entraînés sur des mots par l'intermédiaire de jetons (tokens). C'est le cas des transformeurs génératifs pré-entraînés comme GPT-4, LaMDA, LLaMA, Gemini ou BLOOM. Ils sont entre autres capables de tenir des conversations, de traduire, ou de résumer. BookCorpus (en) et Wikipédia figurent parmi les ensembles de données souvent utilisés pour l'entraînement. Le code informatique étant une forme de texte structuré, les grands modèles de langage peuvent être entraînés sur des langages de programmation. Cela leur permet de générer le code source de nouveaux programmes informatiques.
des images : les systèmes d'IA générative peuvent être entraînés sur des ensembles d'images associés à leur description textuelle. Par exemple avec Imagen, DALL-E, Midjourney, Stable Diffusion ou Flux. Ils sont couramment utilisés pour la génération d'images à partir de texte et pour le transfert de style pictural. Les jeux de données sont notamment LAION-5B et d'autres.
de la musique : les systèmes d'IA générative tels que MusicLM sont entraînés sur les formes d'ondes sonores de la musique enregistrée associée à des annotations textuelles. Ils peuvent générer de nouveaux échantillons musicaux à partir de descriptions textuelles telles qu'« une mélodie de violon apaisante soutenue par un riff de guitare distordu ».
de la vidéo : les systèmes d'IA générative entraînés sur des vidéos annotées peuvent générer des clips vidéo à partir d'une description textuelle. C'est le cas notamment de Gen-4 (Runway AI), Make-A-Video (Meta), Veo (Google DeepMind) ou Sora (OpenAI).
des actions : certains systèmes d'IA peuvent être entraînés un contrôler des robots. Des modèles vision-langage-action (en) comme Gemini Robotics peuvent analyser un flux d'images et réagir à des requêtes textuelles par des séries de mouvements.

Investissements financiers
L'investissement public et privé dans l'IA générative a bondi, à partir du début des années 2020, principalement avec de grandes entreprises telles que Microsoft, Google et Baidu, mais aussi avec de nombreuses petites entreprises développant des modèles d'IA générative.
Les grands acteurs du cloud public cherchent aussi à tirer des revenus de l'IA générative, via des acquisitions et des partenariats stratégiques.
Selon le PitchBook report (mars 2023) de la plateforme sur le capital-risque PitchBook, le marché de l'IA générative devrait atteindre 42,6 milliards de dollars en 2023. Si les chatbots, les assistants virtuels et les voicebots ont capté 57,8 % des investissements en capital-risque dans les interfaces en langage naturel en 2022, les secteurs de la programmation et de la santé devraient aussi générer des revenus importants. Cependant, peu de start-up de l'IA ont éprouvé le besoin d'entrer en bourse en 2023, ce qui a limité les bénéfices à court terme des investisseurs (« baisse de 94 % par rapport à la même période [3 premiers trimestres de l'année] en 2021 »).
Selon un rapport de McKinsey, l'IA générative est en 2023 déjà utilisée par environ un tiers des organisations interrogées, notamment dans les secteurs de la technologie, des médias et du divertissement, de la santé et des services financiers. Elle a un fort potentiel de transformation de l'économie, en augmentant la productivité, l'innovation, la personnalisation et la qualité des produits et services, en créant de nouveaux produits et services ; l'IA générative devrait apporter l'équivalent de 2,6 à 4,4 milliards de dollars par an à l'économie mondiale de 2023 à 2040 (2 à 4 % du PIB mondial), avec des opportunités, mais aussi avec des défis, risques et effets complexes sur le travail, les compétences et la société. Les organisations qui n'investissent pas dans cette technologie risquent de se trouver dépassées.
De son côté J.P. Morgan Private Bank Asia confirme en septembre 2023 des opportunités d'investissement et estime que les secteurs des semi-conducteurs, des cartes graphiques et autres puces de calcul hautement performantes, des fonderies de circuits intégrés, des centres de données, des hyperscalers et des logiciels devraient profiter de l'IA. Cependant, la banque met en garde contre les risques de sécurité, d'éthique, de réglementation et de concurrence, et recommande aux investisseurs une approche prudente et responsable.

Transparence
En 2023, il est reproché aux systèmes d’IA et notamment d'IA générative de ne pas être transparents, autrement dit d'être des « boites noires » dont même les développeurs de l'IA ne comprennent pas le fonctionnement interne.
Une étude laisse penser qu'il pourrait être possible de rendre plus transparente cette boite noire grâce à l’analyse de Fourier appliquée aux réseaux de neurones profonds. Des chercheurs de l’Université Rice ont en effet formé un réseau de neurones profonds à reconnaître les flux complexes d’air ou d’eau et à prédire comment ces flux évoluent. Ils lui ont ensuite appliqué une analyse de Fourier (sur les équations régissant le réseau de neurones). Cette méthode a révélé ce que le réseau de neurones avait appris, et surtout comment il était parvenu à ces connaissances.
De grands concepteurs d’IA comme OpenAI et Meta ont commencé à publier des « Fiches Système » (ou « System Cards », inspirées des « Model Cards », une norme largement acceptée pour la documentation des modèles d’IA). Ces fiches contiennent des informations sur l’architecture et le fonctionnement de leurs IA : objectifs, composants, données, performances, impacts potentiels d’un système d’IA et mesures d’atténuation… C’est une première étape vers une documentation des systèmes d’IA, lesquels combinent souvent plusieurs modèles et technologies interagissant pour accomplir des tâches spécifiques.
Les détecteurs d'IA génératives sont généralement assez peu efficaces sur des textes dans des langues différentes de l'anglais, tout en présentant une notation de certitude faussement précise. Le détecteur d'OpenAI n'identifie ainsi que 26 % des textes écrits par des IA tout en classant par erreur 9 % des textes écrit par des humains comme générés par une IA.

Usages malveillants
En 2024, un employé d'une multinationale de Hong-Kong a été trompé par une visioconférence où tous les participants avaient été créés à l'aide de l'intelligence artificielle pour imiter de vrais collègues. Il a reçu l'ordre d'effectuer des virements, et plus de 25 millions de dollars ont ainsi été détournés.

Régulation et gestion des risques
Différents tests comme HELM ou MMLU permettent d'estimer les capacités ou les comportements indésirables (réponses biaisées, fausses, hallucinations, reprise de contenu protégé par le droit d'auteur...) de grands modèles de langage.
L'Union européenne, les États-Unis et la Chine ont commencé à se doter de législations sur le numérique commençant à prendre en compte l'IA, mais qui s'est avérée dépassée par les progrès rapides de l'IA générative.
Concernant l'éthique de l'intelligence artificielle : en France, le Comité national pilote d'éthique du numérique (CNPEN) a été saisi par le ministre délégué chargé de la Transition numérique et des Télécommunications (20 février 2023) à propos des questions d'éthique « liées à la conception, aux usages, aux impacts sur la société des systèmes d'intelligence artificielle générative ainsi que les accompagnements nécessaires à leur mise en œuvre, en considérant prioritairement la génération automatisée de textes ». Dans son avis du 20 juin 2023 : après avoir fait un rappel des concepts, techniques et vocabulaire des systèmes d'intelligence artificielle générative, le CNPEN a produit une réponse, des préconisations, une analyse des enjeux juridiques en environnementaux ; et des suggestions sur les recherches à conduire dès à présent sur le sujet.
Albert est une intelligence artificielle « souveraine » développée par la Direction interministérielle du numérique (DINUM) pour le compte de l'État français. Son objectif est d'assister les agents publics dans leurs tâches quotidiennes, notamment en facilitant la rédaction de réponses aux demandes des usagers et en automatisant certaines procédures administratives. Cette initiative vise à simplifier et à rendre plus efficace l'action publique au bénéfice des citoyens.

Droits d'auteur
Les œuvres et créations générées par des systèmes d'intelligence artificielle générative soulèvent de nombreuses questions concernant la protection des droits d'auteur.
Aux États-Unis, le 15 mars 2023, l'United States Copyright Office (USCO), qui est responsable de l'enregistrement des droits d'auteurs dans le pays, a annoncé qu'elle refuserait le dépôt d'œuvres uniquement générées par des intelligences artificielles.
En Chine, le tribunal de Shenzhen a considéré que le droit d'auteur d'un texte généré par l'IA appartient à l'éditeur de l'IA.
En France, comme dans l'Union européenne, le droit d'auteur repose sur l'idée que seules les créations issues de l'esprit humain peuvent être protégées. L'article L111-1 du code de la propriété intellectuelle prévoit par exemple que « L'auteur d'une œuvre de l'esprit jouit sur cette œuvre, du seul fait de sa création, d'un droit de propriété incorporelle exclusif et opposable à tous. » Par conséquent, les œuvres créées par une intelligence artificielle devraient porter l'empreinte d'un être humain, au delà d'instructions données à l'IA, pour bénéficier des protections garanties par le code de la propriété intellectuelle.
Au niveau européen, la directive sur le droit d'auteur dans le marché unique numérique (Directive 2019/790) ne prévoit pas de dispositions spécifiques pour les œuvres générées par IA. Toutefois, des débats sont en cours quant à la reconnaissance possible d'une forme de protection juridique pour ces créations, notamment lorsqu'un auteur humain intervient dans le processus de génération.
En réalité, le débat est actuellement aussi bien juridique que philosophique quant au réel propriétaire de l'œuvre : est ce le créateur de l'IA ou l'utilisateur qui a rédigé le prompt ?

Risques existentiels
En 2023, de nombreuses alertes ont été lancées par des pionniers du deep learning (ex. : Geoffrey Hinton, Yoshua Bengio, Sam Altman ou Demis Hassabis). Notamment via une demande de moratoire de 6 mois dans le développement de l'IA (lancé le 28 mars par le Future of Life Institute qui sera signée par plus de 30 000 personnes dont le lauréat du prix Turing Yoshua Bengio et Elon Musk). Puis, en mai 2023, une déclaration du Center for AI Safety (« Centre pour la sûreté de l'IA ») affirmant que « l'atténuation du risque d'extinction de l'humanité lié à l'IA devrait être une priorité mondiale au même titre que la prévention des pandémies et des guerres nucléaires » est signée par d'éminents chercheurs ainsi que par les dirigeants de OpenAI, Google DeepMind et Anthropic.
Selon Heidy Khlaaf (directrice chargée de l'assurance de l'apprentissage automatique chez Trail of Bits, une société de recherche et de conseil en cybersécurité), les centrales nucléaires ont des milliers de pages de documents pour prouver que le système ne cause de tort à personne, et pour Melissa Heikkilä : « la chose la plus importante que la communauté de l'IA pourrait apprendre du risque nucléaire est l'importance de la traçabilité », deux choses encore peu développées dans le secteur de l'IA. La réglementation de l'IA est parfois comparée à la réglementation du secteur nucléaire. Sam Altman (PDG d'OpenAI) a suggéré la mise en place d'un système de licences, dans lequel l'entraînement de systèmes d'IA ayant des capacités élevées nécessiterait l'octroi d'une licence, et pour ce faire de se conformer à des exigences de sécurité (de même que les opérateurs d'installations nucléaires sont tenus d'être licenciés par un régulateur nucléaire). Aidan Gomez (cofondateur de Cohere) a jugé cette formule excessive et détournant l'attention de risques - selon lui plus réels - de l'IA mal utilisée dans les médias sociaux et la médecine, de même que Yann Le Cun (embauché comme scientifique en chef de l'IA chez Meta, le groupe qui détient Facebook et travaille à la création d'un Métavers) et Joëlle Pineau (vice-présidente de la recherche en IA chez Meta) qui jugent ces craintes ridicules et déraisonnables, affirmant que les systèmes d'IA comme ChatGPT ne sont pas encore conscientes et ne peuvent donc pas selon eux manipuler ou détruire l'humanité. Mais en 2023, des chercheurs d'Oxford, de Cambridge, de l'Université de Toronto, de l'Université de Montréal, de Google DeepMind, d'OpenAI, d'Anthropic, de plusieurs organismes de recherche à but non lucratif sur l'IA et Yoshua Bengio (lauréat du prix Turing) suggèrent dans un article que les créateurs des modèles d'IA les plus puissants doivent pouvoir évaluer si ces modèles ont des capacités pouvant présenter des risques « extrêmes » (planification à long terme, manipulation, auto-prolifération, conscience de la situation, capacités à mener des cyberattaques ou à acquérir des armes notamment biologiques...). Les développeurs doivent également apprendre à mesurer la propension des modèles à appliquer leurs capacités à nuire (ceci peut se faire grâce à des « évaluations d'alignement »). Selon eux, « ces évaluations deviendront cruciales pour informer les décideurs politiques et les autres parties prenantes, et pour prendre des décisions responsables concernant l'entraînement, le déploiement et la sécurité des modèles d'IA ».
Rishi Sunak (premier ministre britannique) a invité la communauté internationale à Bletchey park en novembre 2023 pour un premier sommet mondial en sûreté de l'IA, abordant notamment les risques existentiels liés à l'IA. Sunak propose que soit créé un groupe d'experts internationaux, sur le modèle du GIEC, qui serait dans un premier temps chargé de publier un état des lieux de l'IA. Le 1er novembre, la Chine, les États-Unis, l'Union européenne et une vingtaine de pays ont signé la déclaration de Bletchley pour un développement « sûr » de l'intelligence artificielle. La veille, trois de ces pays (France, Allemagne et Italie) avaient signé à Rome un accord de coopération sur l'IA « dans le prolongement des efforts globaux déployés en faveur de la transition numérique et écologique ».

Explicabilité
Aleksander Mądry (professeur d'informatique au Cadence Design Systems du MIT, et directeur du Center for Deployable Machine Learning du MIT) a été audité en mars 2023 par le Sous-comité sur la cybersécurité, les technologies de l'information et l'innovation gouvernementale lors d'une cession intitulée « Progrès de l'IA : sommes-nous prêts pour une révolution technologique ? ». Selon Mądry, « nous sommes à un point d'inflexion en ce qui concerne ce que l'IA du futur apportera [...] le gouvernement devrait plutôt s'interroger sur l'objectif et l'explicabilité des algorithmes utilisés par les entreprises, en tant que précurseur à la réglementation » pour s'assurer que l'IA est cohérente avec les objectifs de la société. Ce serait une erreur selon lui de réglementer l'IA comme si elle était humaine – par exemple en demandant à l'IA d'expliquer son raisonnement et en supposant que les réponses qui en résultent sont fiables.

Emploi
Quelques nouveaux métiers apparaissent (architecte de l'IA, manager d'algorithmes...). Cependant, le risque de suppression massive d'emplois préoccupe, en particulier pour les métiers intellectuels mais aussi pour les métiers manuels. Les progrès de l'IA et de la robotique avancée, avec des capteurs d'informations sensorielles et des algorithmes décisionnels avancés, permettent l'automatisation de nombreuses tâches avec des niveaux d’agilité et d’adaptation comparables et parfois meilleurs que ceux des humains. Infirmiers, plombiers, ouvriers spécialisés, artisans et artistes, ainsi que d'autres professions manuelles complexes qui exigent une prise de décision en contexte d’incertitude et de la dextérité, risquent d'être confrontés à des machines de plus en plus autonomes, capables de les remplacer[réf. souhaitée]. OpenAI a publié en 2024 les résultats d'une étude sur les effets d'un revenu universel.  
En France, un Observatoire des Emplois Menacés et Émergents (OEM) a été créé avec 3 missions principales :  

« identifier les emplois les plus exposés à court, moyen et long terme, en analysant l'évolution des tendances et des scénarios d'impact de l'IA », en tenant compte de l'évolution de la demande de biens et de services liés à chaque type d'emploi aux échelles microéconomique (analyse détaillée des tâches que l'IA peut remplir) et macroéconomique (effets de demande) pour comprendre et anticiper les dynamiques du marché du travail.
« Cartographier les secteurs et les régions les plus vulnérables, afin d'anticiper les bouleversements économiques et territoriaux et d'éclairer la prise de décision »
« Imaginer quels seront les nouveaux métiers qui émergeront avec la nouvelle organisation de la production »

Concurrence
En février 2024, l'Autorité de la concurrence s'autosaisit et lance un consultation publique des acteurs de l'IA générative en vue d'émettre un avis, rendu le 28 juin 2024.

L'Autorité y souligne la main mise croissante du privé sur ce marché ; elle encourage le soutien dans toute l'Europe à un accès du privé local aux supercalculateurs publics, tels que, en France, le supercalculateur Jean Zay), en échange de produits open ; et tout en maintenant une priorité aux recherches académiques.
L'Autorité s'inquiète de l'importance des « barrières à l'entrée » pour les nouveaux entrants de l'IAg. Ces derniers manquent d'accès aux puissances nécessaires de calcul, à d'énormes volumes de données (pour entrainer les modèles), au recrutement de talents rares et coûteux, que les GAFAM américains captent, grâce à des avantages concurrentiels exorbitants (intégration verticale et maitrise des marchés connexes, verrouillage dans les services Cloud, accords non transparents d'exclusivité, prises de participation minoritaires) et fixations arbitraires et déloyales de prix, de restrictions de production et de conditions contractuelles déloyales, comportements discriminatoires. Les IAg comme Anthropic, Mistral, Open AI, Perplexity… sont toutes au moins en partie financées par Google, Amazon, Microsoft, Intel, Salesforce et/ou Nvidia. L'Autorité note que le secteur est dépendant du « logiciel de programmation de puces CUDA de Nvidia (seul environnement parfaitement compatible avec les GPU devenus incontournables pour le calcul accéléré)] » et s'inquiète des « investissements de Nvidia dans des fournisseurs de services cloud spécialisés dans l'IA, tels que Coreweave », un secteur scruté par l'Autorité (cf. visite et saisie inopinée en septembre 2023). En intégrant l'IAG dans leurs écosystèmes de produits et services - comme le fait Microsoft avec Copilot - les GAFAM renforcent leur position dominante. L'Autorité de la concurrence note que « les investissements dans le secteur ont été multipliés par près de six entre 2022 et 2023. Les entreprises du secteur ont ainsi levé plus de 22 milliards de dollars en 2023 (soit environ 20 milliards d'euros), contre environ 4 milliards de dollars en 2022 (soit environ 3,7 milliards d'euros) ». L'Autorité suggère notamment, à la commission européenne, d'exiger, via le DMA, la transparence sur les participations minoritaires. Pour l'Autorité, les IAg entraînées sur des supercalculateurs publics devraient se voir imposer des critères d'ouverture ; et il faut inventer un équilibre entre une juste rémunération des ayants-droit et l'accès des développeurs d'IA aux données nécessaires pour innover.
De plus, les données francophones utilisées pour entraîner les LLM ne compte que pour 0,2 % du total. Face à ce taux qui est source de biais culturels anglo-saxons dans l'IA et d'appauvrissement langagier, la France a lancé le projet Villers-Cotterêts (Cité internationale de la langue française) qui doit notamment ouvrir un large corpus numérique francophone (et de langues régionales), via une base centralisée ouverte à l'apprentissage des IA (dont Albert, l'IA de l'État). Ce corpus est construit à partir des Archives de France, de la Bibliothèque nationale de France, du CNRS et de diverses ONG.Un autre projet dit Argimi (Artefact, Giskard et Mistral), soutenu par l'AAP « Communs numériques pour l'intelligence artificielle générative », est porté par un consortium de startups souhaitant ouvrir le patrimoine de la BNF et de l'INA à l'apprentissage de LLM créés par ces trois start-ups françaises.

Notes et références
Voir aussi
Articles connexes
Arts de l'intelligence artificielle
Modèle texte-image, Modèle textuel-vidéo
Falcon 180B
Grand modèle de langage
Google Gemini
Recherche générative assistée par intelligence artificielle
Réseau antagoniste génératif
Slop (intelligence artificielle)
Transformateur génératif pré-entraîné
Décret présidentiel 14179

 Portail de l’intelligence artificielle   Portail de l’informatique   Portail des années 2020   Portail de l’imagerie numérique   Portail de l’écriture