Une intelligence artificielle générale (IAG) est une intelligence artificielle capable d'effectuer ou d'apprendre pratiquement n'importe quelle tâche cognitive au moins aussi bien que l'humain. La création d'intelligences artificielles générales est un des principaux objectifs de certaines entreprises comme OpenAI, Meta et DeepMind. C'est aussi un thème majeur de la science-fiction et de la futurologie. Même si GPT-4 a été décrit comme ayant des « étincelles d'intelligence artificielle générale », il n'existe pas en 2024 d'IA consensuellement considérée comme générale.
Bien que l'intelligence artificielle générale puisse être très utile dans de nombreux domaines, elle présente également des risques, notamment de désinformation et de chômage de masse. Selon des experts en IA comme Stuart Russell, Yoshua Bengio ou Geoffrey Hinton et d'autres chercheurs en IA, une perte de contrôle pourrait également causer des risques existentiels tels que la fin de l'humanité.

Terminologie
Selon la plupart des spécialistes, l'IAG (Artificial general intelligence ou AGI en anglais), ou IA de niveau humain, se réfère à la capacité d'une machine autonome à effectuer l’ensemble des tâches intellectuelles qu'un être humain peut effectuer. Cette définition assez restrictive fait qu'une IA peut générer du texte de haute qualité et être bien plus rapide que l'humain sans être qualifiée de « générale », s'il reste des tâches qu'elle n'effectue pas aussi bien que l'humain. OpenAI définit plutôt l'IAG comme un système hautement autonome capable de surpasser l'humain dans la plupart des tâches ayant un intérêt économique.
Un système expert, ou IA « étroite » est un système informatique potentiellement très compétent, mais qui n'opère que dans un contexte restreint, souvent focalisé sur une tâche précise.
Le terme d'IA forte fait quant à lui plus souvent intervenir une notion philosophique de conscience, ce qui fait que les capacités de l'IA ne suffisent pas à dire si elle est « forte ». Une IA peut donc être parfois qualifiée de « faible » dans ce sens, indépendamment de ses compétences, si elle n'est pas « consciente ». Ces notions font référence à l'hypothèse de l'IA forte et de l'IA faible dans l'expérience de pensée de la chambre chinoise.
Le terme de superintelligence artificielle décrit une IA aux capacités intellectuelles bien supérieures à celles de l'humain dans pratiquement tous les domaines. Les experts affichent une large incertitude sur le temps que cela prendrait de passer d'une IAG à une superintelligence artificielle, les estimations pouvant aller de moins d'une heure à plusieurs décennies, en supposant que ce soit possible.
Cela dit, ces termes peuvent avoir des définitions différentes, et ces définitions sont suffisamment vagues pour qu'il puisse parfois être difficile de catégoriser une IA.

Histoire
Dans les années 1950, la première génération de chercheurs en intelligence artificielle était convaincue que l'IAG est possible et existerait au bout de quelques décennies. Le pionnier Herbert Simon a écrit en 1965 : « les machines seront capables, dans moins de 20 ans, d'accomplir n'importe quelle tâche qu'un homme peut accomplir ».
Cependant, dans les années 1970, il devint évident qu'ils avaient grossièrement sous-estimé la difficulté du projet. Les fonds d'investissement devinrent sceptiques en ce qui concernait l'IAG et mirent davantage de pression sur les chercheurs pour obtenir des applications plus concrètes. Après un bref regain d'intérêt dans les années 1980, la confiance générale dans l'IA rechute. Jusqu'aux années 1990, les chercheurs en IA étaient réputés pour faire des promesses vaines, et devinrent réticents à faire la moindre prédiction.
Dans les années 1990 et au début du XXIe siècle, la recherche en IA a atteint un certain succès commercial et une respectabilité académique en se concentrant sur des applications commerciales ou des sous-problèmes auxquels l'IA peut fournir des résultats vérifiables. Notamment via des réseaux de neurones artificiels ou des techniques d'apprentissage statistique. Ces techniques sont maintenant largement appliquées dans l'industrie.
Le terme d'« intelligence artificielle générale » a été utilisé dès 1997 par Mark Gubrud dans une discussion sur les implications de l'automatisation de la production et des opérations militaires.
En 2016, DeepMind a conçu AlphaGo, un programme d'apprentissage par renforcement qui s'est avéré capable de vaincre le champion du monde de Go Lee Sedol. Ce programme nécessitait cependant une connaissance spécifique du jeu de Go et des données humaines, ainsi que des règles du jeu. En 2020, Google DeepMind crée MuZero, qui n'a plus ces limitations et peut être entraîné sur d'autres types de jeux comme les jeux Atari. En 2022, Google DeepMind développe Gato, un système d'IA généraliste capable d'exécuter plus de 600 tâches.
En 2023, Microsoft Research a publié une étude sur une version précoce de GPT-4, affirmant que GPT-4 manifestait une intelligence plus générale que les précédents modèles d'IA, et affichait des performances de niveau humain dans de multiples domaines tels que les mathématiques, la programmation et le judiciaire. Une autre étude en 2023 rapporte que GPT-4 surpasse 99 % des humains aux tests de créativité de Torrance.
L'année 2023 a également marqué l'émergence des grands modèles multimodaux, qui sont capables de traiter ou de générer différents types de données comme le texte, le son ou les images.
En janvier 2024, Mark Zuckerberg, directeur général de Meta, annonce que son entreprise intensifie ses travaux pour s’attaquer non seulement à l'IA, mais aussi à l’IA générale. Mark Zuckerberg souhaite créer des systèmes informatiques beaucoup plus intelligents, capables d’égaler au moins les capacités cognitives humaines telles que l’apprentissage, le raisonnement, la planification, la création et la mémorisation d’informations.
En 2024, OpenAI lance o1-preview, le premier d'une série de modèles qui « prennent plus de temps pour réfléchir avant de répondre ». Selon Mira Murati, cette capacité à réfléchir avant de répondre représente un paradigme supplémentaire. Elle améliore les résultats du modèle en dépensant de la puissance de calcul supplémentaire lors de la génération de la réponse. L'autre paradigme, qui reste toujours pertinent, consiste à entraîner des modèles plus grands, utilisant davantage de données et de puissance de calcul pour leur entraînement.

Caractéristiques
Une intelligence artificielle générale (IAG) doit notamment pouvoir :

planifier ;
comprendre des concepts abstraits ;
résoudre des problèmes ;
prendre des décisions en tenant compte de l'incertitude ;
apprendre ;
communiquer en langage naturel ;
faire preuve de créativité.

Sentience
La sentience n'est en général pas considérée comme un critère nécessaire pour l'IAG. Une machine pourrait probablement en effet être très compétente pour accomplir des objectifs sans nécessairement être sentiente.
La question de savoir si les machines peuvent en principe être sentientes est controversée. Il n'existe actuellement pas de mesure fiable de la sentience, ni même de théorie qui fasse largement consensus. Selon les fonctionnalistes, la sentience est causée par certains types de traitement de l'information. Il n'y a dans ce cas pas de barrière théorique au fait que la sentience puisse se manifester sur un support autre que biologique. Le fait qu'une machine soit sentiente n'implique pas nécessairement qu'elle soit plus puissante ou plus dangereuse, mais plutôt qu'elle pourrait avoir une dimension morale. Si c'est le cas, il pourrait, de même que pour les animaux, être pertinent d'accorder des droits à ces IAs ou de se soucier de leur bien-être.

Explosion d'intelligence
Une IAG serait en principe capable de remplacer le poste de chercheur en intelligence artificielle. Cependant, les machines étant en général plus rapides que le cerveau humain, certains chercheurs estiment qu'il pourrait survenir une « explosion d'intelligence », faisant rapidement de l'IAG une superintelligence.
Le concept n'est pas nouveau. En 1965, Irving John Good écrivait :

« Supposons une machine ultra-intelligente définie comme une machine pouvant surpasser toutes les activités intellectuelles de n'importe quel homme quelle que soit son intelligence. La conception de telles machines étant l’une de ces activités intellectuelles, une machine ultra-intelligente pourrait concevoir des machines encore meilleures ; il y aurait alors incontestablement une « explosion d’intelligence », et l’intelligence humaine serait très vite laissée loin derrière. Ainsi, l’invention de la première machine ultra-intelligente est la dernière invention que l’humanité ait besoin de réaliser, à condition que la machine soit suffisamment docile pour nous dire comment la garder sous contrôle. »

— Irving John Good, Speculations Concerning the First Ultraintelligent Machine
Ce scénario est débattu. D'autres pensent que cette amélioration récursive se ferait de façon plus décentralisée ou progressive, s'étalant sur des années voire des décennies. L'économiste Robin Hanson estime par exemple que pour déclencher une explosion d'intelligence, une machine devrait avoir une meilleure capacité d'innovation que le reste du monde combiné, ce qui lui semble peu plausible.

Faisabilité
Là où l'approche de Google DeepMind avec AlphaGo, Muzero et Gato consistait à partir de systèmes d'apprentissage par renforcement puissants mais spécifiques et à tenter de les généraliser à des situations de plus en plus complexes (avec un certain succès), les grands modèles de langage ont récemment fournit une approche différente. Initialement, les modèles d'IA disposent d'un grand nombre de paramètres ajustables (par exemple, 175 milliards dans le cas de GPT-3). La première étape de l'entraînement des grands modèles de langage consiste à prédire, pour un très grand nombre de textes issus d'internet, le mot suivant (ou plus exactement le token suivant, un token étant une séquence d'octets). Au fur et à mesure de cet apprentissage, le modèle ajuste ses paramètres internes. Cette tâche étonnamment simple, répétée un grand nombre de fois, permet au modèle de langage d'acquérir une connaissance non seulement du langage, mais aussi de nombreux autres domaines tels que l'arithmétique, le raisonnement, la physique, l'humour, la traduction…
Une procédure d'entraînement relativement simple est ainsi capable de faire émerger des connaissances et des capacités complexes. L'IAG pourrait peut-être ainsi être principalement une question de quantité de ressources consacrées à l'entraînement. Il semble en effet que les performances augmentent régulièrement avec :

le nombre de paramètres du modèle (sa taille) ;
la quantité de données ayant servi à entraîner du modèle ;
les capacités de calcul ayant servi à entraîner le modèle, ou autrement dit le nombre d'itérations.
Il y a quelques années, l'intelligence artificielle générale était perçue comme un sujet spéculatif et lointain. En 2015, le vice-président de Baidu, Andrew Ng, affirmait que s'inquiéter des risques existentiels liés à l'IAG est « comme s'inquiéter de la surpopulation sur Mars alors que nous n'avons même pas encore mis le pied sur la planète ».
Les spectaculaires progrès récents suggèrent cependant que l'IAG pourrait être moins difficile à réaliser que prévu. Selon l'informaticien Geoffrey Hinton :

« L'idée que ça pourrait devenir plus intelligent que l'humain — quelques gens y croyaient, [...]. Mais la plupart pensaient que ce ça n'arriverait pas avant longtemps. Et je pensais que ça n'arriverait pas avant longtemps. Je pensais que ça prendrait entre 30 et 50 ans. Évidemment, ce n'est plus ce que je pense. »

— Geoffrey Hinton
Certains estiment que GPT-5, le successeur à venir de GPT-4, sera probablement une intelligence artificielle générale. La définition d'intelligence artificielle générale reste cependant suffisamment floue pour qu'il puisse être difficile ou subjectif d'évaluer si un modèle est une IAG.

Bénéfices
L'IAG pourrait avoir des applications très variées. Si elle est orientée vers ce but, l'IAG, et plus encore la superintelligence artificielle, pourrait aider à combattre divers problèmes dans le monde tels la faim, la pauvreté et les problèmes de santé.
L'IAG pourrait augmenter la productivité et l'efficacité dans la plupart des travaux. Par exemple, dans le secteur de la santé, l'IAG pourrait accélérer la recherche, notamment contre le cancer. Elle pourrait s'occuper des personnes âgées, ou encore démocratiser l'accès à des diagnostics médicaux rapides et de qualité. De même dans le secteur de l'éducation, où elle pourrait offrir une éducation ludique, personnalisée et à faible coût. Pour pratiquement n'importe quel travail bénéficiant à la société s'il est bien accompli, il serait probablement tôt ou tard préférable de le laisser à une IAG. Le besoin de travailler pour subsister pourrait devenir obsolète, si les richesses produites sont adéquatement redistribuées. Cela pose donc aussi la question de la place de l'humain dans une société radicalement automatisée.
L'IAG pourrait aussi aider à prendre des décisions rationnelles, anticiper et empêcher des catastrophes. Si une IAG avait pour objectif premier d'empêcher des catastrophes planétaires telles que l'extinction de l'humanité (ce qui pourrait être difficile si l'Hypothèse du Monde Vulnérable s'avère fondée), elle pourrait ainsi prendre des mesures réduisant drastiquement les risques tout en minimisant l'impact de ces mesures sur notre qualité de vie. Elle pourrait aussi aider à tirer les bénéfices de technologies potentiellement catastrophiques telles que les nanotechnologies ou l'ingénierie climatique tout en évitant les risques associés.

Risques
Les risques associés à l'IAG sont divers et peuvent provenir de l'IAG elle-même, ou du comportement humain face à celle-ci. Certains risques sont déjà un problème aujourd'hui, comme le chômage, la désinformation ou les armes létales autonomes, mais pourraient être exacerbés par l'IAG. En outre, la capacité de la société humaine à prédire et à contrôler le développement de l'IAG, ainsi que sa capacité à faire face aux risques potentiels font l'objet d'études.
Voici une liste non-exhaustive de risques importants.

Perte de contrôle
Un risque particulièrement débattu est celui d'une catastrophe existentielle liée l'IAG. Cela implique en général une perte irréversible de contrôle d'une IAG qui aurait un objectif différent de celui des humains.
Le sort de l'humanité face à l'IAG a parfois été comparé à celui du gorille, menacé par les activités humaines. Le supplément d'intelligence de l'humanité l'a mise dans une position de domination, et le gorille est devenu vulnérable de façons qu'il n'aurait pas pu anticiper. Le gorille est devenu une espèce menacée, pas par malveillance, mais simplement comme un dommage collatéral des activités humaines.
Le fait d'assigner les objectifs souhaités à des intelligences artificielles générales est encore un sujet actif de recherche, baptisé le problème de l'alignement. Selon OpenAI, l'IAG pose un risque existentiel, et « résoudre le problème de l'alignement des IAG pourrait s'avérer si difficile que cela nécessiterait que toute l'humanité y travaille ensemble ». Une des difficultés vient du fait que si l'IAG est capable de manipuler des humains, il est très difficile de vérifier si ses intentions sont vraiment celles que l'on souhaite. Une approche nommée l'« interprétabilité » consiste à tenter de comprendre le fonctionnement interne de ces modèles d'IA, notamment afin de pouvoir détecter des signes de manipulation, une tâche difficile vu la complexité et le nombre de paramètres de ces modèles d'IA.
Des sceptiques comme Yann Le Cun estiment que les intelligences artificielles générales n'auront pas de raison de vouloir dominer l'humanité, et qu'il faut être prudent de ne pas les anthropomorphiser et leur accorder des intentions humaines. Il considère que les humains ne seraient pas « suffisamment intelligents pour concevoir des machines superintelligentes, et pourtant ridiculement stupides au point de leur donner des objectifs idiots sans avoir de garde-fous ».

Le principe de convergence instrumentale suggère à l'inverse que presque quel que soit les objectifs d'un agent intelligent, celui-ci a des raisons de chercher à survivre et accumuler davantage de ressources et d'influence, car cela l'aide à accomplir ces objectifs. Et que cela ne nécessite pas que l'IA ait des émotions. Le philosophe Nick Bostrom donne l'exemple du maximiseur de trombones pour illustrer comment l'optimisation d'un objectif quelconque pourrait avoir des conséquences catastrophiques : 
« Supposons que nous ayons une IA dont l'unique but soit de faire autant de trombones que possible. L'IA se rendra vite compte que ce serait bien mieux s'il n'y avait pas d'humains, parce que les humains pourraient décider de l'éteindre. Parce que si les humains le faisaient, il y aurait moins de trombones. De plus, le corps humain contient beaucoup d'atomes qui pourraient être transformés en trombones. L'avenir vers lequel l'IA essaierait de se diriger serait un futur avec beaucoup de trombones mais aucun humain. »

— Nick Bostrom Beaucoup de sceptiques estiment que l'IAG n'est pas pour bientôt ou n'aura pas lieu, ou que ces risques contre-intuitifs détournent l'attention de problèmes plus concrets et immédiats.
En 2023, Max Tegmark dresse une analogie entre la superintelligence non alignée et la comète qui s'apprête à heurter la Terre dans le film à succès Don't Look Up.

Chômage de masse
Les modèles de langage actuels pourraient déjà toucher le marché de l'emploi. Selon OpenAI, « 80 % de la main-d'œuvre américaine pourrait voir au moins 10 % de ses tâches affectées, tandis qu'environ 19 % des travailleurs pourraient voir au moins 50 % de leurs tâches affectées par l'introduction d'outils tels que ChatGPT ». Les travailleurs de bureau semblent être les plus concernés, par exemple les mathématiciens, comptables ou concepteurs de sites Web.
L'IAG pourrait augmenter l'autonomie de ces systèmes, leur capacité à prendre des décisions, à s'interfacer avec d'autres outils informatiques, mais aussi à contrôler des corps robotisés.
Cette automatisation est parfois considérée comme une aubaine, créant davantage de richesses et allégeant les charges de travail. Larry Page, cofondateur de Google, estime que cela permettra aux humains de « vivre mieux » en nous « libérant du temps ». Selon Stephen Hawking, le résultat dépendra notamment de la façon dont les richesses seront redistribuées :

« Chacun pourra vivre une voluptueuse existence de loisirs si les richesses produites par les machines sont réparties, ou bien la majorité des gens vivront dans la misère si les propriétaires de ces machines parviennent à nullifier la redistribution des richesses. Jusqu’ici, la tendance semble être la seconde option, et la technologie creuse toujours plus les inégalités. »

— Stephen Hawking
D'après Elon Musk, cette automatisation générale des tâches dans la société nécessitera l'adoption par les gouvernements d'un revenu universel.

Désinformation et manipulation
La génération automatique d'images ou de textes crédibles est déjà possible. Par exemple, une fausse image d'une explosion du Pentagone le 22 mai 2023, partagée par des comptes financiers ainsi que par des médias russes, a entraîné une baisse momentanée de 0,3 % des cours boursiers aux États-Unis. L'IAG pourrait néanmoins acquérir une meilleure compréhension du contexte, interagir avec d'autres outils et se montrer plus convaincante, stratégique, personnalisée et autonome.
La capacité des modèles d'IA à produire du contenu très rapidement et à faible coût pourrait inonder internet de texte, d'images ou de vidéos générés automatiquement. Ce contrôle de l'information pourrait également être exploité par des régimes autoritaires à des fins de surveillance et de contrôle. Cela pourrait ainsi intensifier la diffusion de désinformation et de propagande. Une solution proposée serait l'adoption à grande échelle de systèmes numériques permettant de prouver anonymement sur internet que l'on est un humain, ce qui permettrait notamment de filtrer efficacement les faux comptes.

Cyberattaques et prolifération
L'IAG peut aussi automatiser les cyberattaques ou les escroqueries reposant sur l'ingénierie sociale. Que ce soit l'intention du concepteur ou non, il se pourrait que l'IAG soit en mesure d'exploiter les faiblesses de systèmes informatiques pour déjouer des protections, obtenir de l'argent ou se reproduire.

Notes et références
Voir aussi
Articles connexes
Alignement des intelligences artificielles
ChatGPT
Intelligence artificielle quantique
Grand modèle de langage
Risque de catastrophe planétaire lié à l'intelligence artificielle générale
Superintelligence
Association for the Advancement of Artificial Intelligence
Ben Goertzel
OpenCog

Liens externes
Site officiel de l'AGI Society

Vidéographie
(en) David Shapiro, « AGI in 7 Months! Gemini, Sora, Optimus, & Agents - It's about to get real weird out there! » [« AGI en 7 mois ! Gemini, Sora, Optimus et Agents intelligents – Tout ceci et en train de devenir vraiment étrange »], sur YouTube, 16 février 2024 (consulté le 18 février 2024).

 Portail de l’intelligence artificielle   Portail de l’informatique   Portail de la psychologie