La réglementation de l'intelligence artificielle est l'élaboration de politiques et de lois du secteur public pour promouvoir et réglementer l'intelligence artificielle (IA). Elle est donc liée à la réglementation des algorithmes. Le paysage réglementaire et politique de l'IA est un problème émergent dans les juridictions du monde entier, y compris dans l'Union européenne (qui dispose d'un pouvoir de réglementation gouvernemental) et dans les organismes internationaux comme l'IEEE, l'OCDE (qui n'en ont pas) et d'autres. Depuis 2016, une série de lignes directrices en matière d’éthique de l'IA ont été publiées afin de maintenir le contrôle social sur cette technologie. La réglementation est considérée comme nécessaire à la fois pour encourager l’IA et pour gérer les risques associés. Outre la réglementation, les organisations déployant l’IA jouent un rôle central dans la création et le déploiement d’une IA digne de confiance, et dans l'atténuation des risques. La réglementation de l’IA par le biais de mécanismes tels que les commissions d’examen peut également être considérée comme un moyen social d’aborder le problème du contrôle de l’IA.
Selon AI Index de Stanford, le nombre annuel de lois liées à l'IA adoptées dans les 127 pays étudiés est passée de une en 2016 à 37 adoptées rien qu'en 2022.

Contexte
En 2017, Elon Musk a appelé à une réglementation du développement de l’IA. Selon NPR, le PDG de Tesla n'était « clairement pas ravi » à l'idée de plaider en faveur d'un contrôle gouvernemental susceptible d'impacter son propre secteur, mais il estimait que les risques d'un manque de contrôle étaient trop élevés : « En général, des règlements sont mis en place après que des événements se soient produits [...] Cela prend une éternité. Cette manière de faire n’est pas une bonne chose car nous agissons toujours après coup (par réaction). Par le passé, heureusement, cela n’a jamais représenté un risque fondamental pour l’existence de notre civilisation. Concernant l’IA en revanche, c’est différent. Elle représente une véritable menace pour notre existence. » En réponse, certains politiciens ont exprimé leur scepticisme quant à la sagesse du fait de réglementer une technologie encore en développement. Répondant à la fois à Elon Musk et aux propositions de février 2017 des législateurs de l’Union européenne visant à réglementer l’IA et la robotique, le PDG d’Intel, Brian Krzanich, avait déclaré que l’IA n'en était encore qu'à ses débuts et qu’il était trop tôt pour réglementer cette technologie. Au lieu d’essayer de réglementer la technologie elle-même, certains chercheurs ont suggéré d’élaborer des normes communes incluant des exigences en matière de test et de transparence des algorithmes.
Dans une enquête Ipsos de 2022, les attitudes à l’égard de l’IA variaient considérablement selon les pays. 78 % des citoyens chinois et seulement 35 % des Américains conviennent que « les produits et services utilisant l'IA présentent plus d'avantages que d'inconvénients ». Un sondage Reuters/Ipsos de 2023 a révélé que 61 % des Américains sont d’accord, et 22 % ne sont pas d’accord sur le fait que l’IA présente des risques pour l’humanité. Dans un sondage Fox News de 2023, 35 % des Américains pensaient qu'il était « très important » et 41 % supplémentaires pensaient qu'il était « assez important » que le gouvernement fédéral réglemente l'IA, contre 13 % répondant « pas très important » et 8 % répondant « pas du tout important ».

Définitions d'intelligence artificielle
Il est difficile de trouver une définition précise et actionnable du terme « intelligence artificielle ».
Pour l'OCDE, un système d'IA est « un système basé sur une machine qui, pour des objectifs explicites ou implicites, déduit, à partir des informations qu’il reçoit, comment générer des résultats tels que des prédictions, du contenu, des recommandations ou des décisions, qui peuvent influencer les environnements physiques ou virtuels. Les différents systèmes d’IA varient dans leurs niveaux d’autonomie et d’adaptabilité après leur déploiement ».
L'une des définitions admises par la Commission européenne est « une technologie qui permet aux machines et aux ordinateurs d’effectuer des tâches qui nécessiteraient autrement une intelligence ou une intervention humaine. L’IA est basée sur des algorithmes qui modélisent le processus de prise de décision du cerveau humain, et qui peuvent apprendre des données disponibles afin de faire des prédictions de plus en plus précises ». Après une une analyse des aspects technico-économiques de l'IA en 2020 s'appuyant sur 65 études scientifiques, le Centre commun de recherche (CCR, organe de la Commission européenne consacré à la veille sur l’intelligence artificielle digne de confiance et qui suit la taxonomie du domaine) a échoué a synthétiser une définition commune ou consensuelle.
Le règlement sur l’intelligence artificielle (AI Act), adopté par le Parlement européen et le Conseil européen, entré en vigueur le 13 juin 2024, offre une définition plus précise et opérationnelle. L'IA est considéré comme « un système automatisé qui est conçu pour fonctionner à différents niveaux d’autonomie et peut faire preuve d’une capacité d’adaptation après son déploiement, et qui, pour des objectifs explicites ou implicites, déduit, à partir des entrées qu’il reçoit, la manière de générer des sorties telles que des prédictions, du contenu, des recommandations ou des décisions qui peuvent influencer les environnements physiques ou virtuels ». Cette définition légale européenne sert de base pour classer les systèmes d'IA en fonction de leurs risques et d'établir des règles spécifiques pour chaque catégorie.

Besoin
La réglementation des intelligences artificielles consiste en l’élaboration de politiques et de lois du secteur public visant à promouvoir et à réglementer l’IA. La réglementation est désormais généralement considérée comme nécessaire pour encourager l’IA et gérer les risques associés. Les considérations en matière d’administration publique et de politique se concentrent généralement sur les implications techniques et économiques, sur l'IA digne de confiance, et sur l'IA centrée sur l'humain ; bien que la réglementation des superintelligences artificielles soit également prise en compte. L'approche de la réglementation en matière d'IA se concentre couramment sur les risques et biais des algorithmes d'apprentissage automatique, notamment sur les données utilisées, le test des algorithmes, le choix du modèle de décision, ainsi que sur l'explicabilité des résultats.
Les propositions de réglementation de l'IA peuvent être plus ou moins contraignantes. Certains juristes ont souligné que les approches juridiquement contraignantes (hard laws en anglais) présentent des défis considérables en réglementation de l’IA. Notamment du fait que les technologies d'IA évoluent rapidement, les approches législatives traditionnelles ne pouvant pas suivre le rythme des applications émergentes et des risques et bénéfices associés. De même, la diversité des applications de l’IA constitue un défi pour les agences de réglementation, dont la portée juridictionnelle est souvent limitée. En guise d’alternative, certains juristes soutiennent que les approches moins contraignantes (droit mou) sont prometteuses pour réglementer l'IA, car elles sont plus flexibles et évolutives. Le manque de contrainte peut cependant aussi être un problème.
Cason Schmit, Megan Doerr et Jennifer Wagner ont proposé la création d'un organisme de réglementation quasi-gouvernemental, qui tirerait parti des droits de propriété intellectuelle avec des licences contaminantes de type copyleft. Ces licences porteraient par exemple sur les modèles d'IA ou les données utilisées pour l'entraînement du modèle, et permettraient à l'organisme désigné de mettre en vigueur un code de conduite ou un ensemble de principes éthiques.
La réglementation de l’IA pourrait découler de principes de base. Une méta-analyse de 2020 a identifié 8 principes de base: confidentialité, imputabilité, sûreté et sécurité, transparence et explicabilité, équité et non-discrimination, contrôle de la technologie par l'humain, responsabilité professionnelle et respect des valeurs humaines. Les lois et réglementations sur l’IA ont été divisées en trois sujets principaux, à savoir la gouvernance des systèmes intelligents autonomes, la responsabilité et l’imputabilité, et les questions de confidentialité et de sûreté. Une approche d'administration publique voit une relation entre la loi et la réglementation de l'IA, éthique de l'IA, acceptation sociale, confiance dans l'IA, et la transformation de l'interaction homme-machine et de la force de travail. L’élaboration de stratégies du secteur public pour la gestion et la réglementation de l’IA est jugée nécessaire aux niveaux local, national et international. Elle s'applique dans divers domaines tels que la gestion et l'imputabilité des services publics, l'application de la loi, le médical, le secteur financier, la robotique, les véhicules autonomes, l'armée, la sécurité nationale et le droit international.
Henry Kissinger, Eric Schmidt et Daniel Huttenlocher ont publié une déclaration commune en novembre 2021 intitulée Being Human in an Age of AI (« Être humain à l'ère de l'IA »), appelant à la création d'une commission gouvernementale pour réglementer l'IA.

En réponse au problème du contrôle de l'IA
La régulation de l’IA peut être considérée comme un bon moyen social de gérer le problème du contrôle de l'IA (la nécessité d’assurer une IA bénéfique à long terme). D’autres approches telles que ne rien faire ou interdire l'IA sont en général considérées comme peu adaptées. Certaines approches transhumanistes comme l’amélioration des capacités humaines (par exemple avec des interfaces cerveau-machine) sont parfois considérées comme complémentaires. La réglementation de la recherche sur l'intelligence artificielle générale (IAG) se concentre sur le rôle des comités d'examen (des universités et entreprises jusqu'aux niveaux internationaux), sur le soutien de la recherche en sûreté de l'IA, sur la possibilité de développement intellectuel différentiel (en privilégiant les stratégies protectrices aux stratégies risquées dans le développement de l’IA) ou sur la possibilité de mener une surveillance internationale pour contrôler le développement de l’IAG. Par exemple, le concept d'AGI Nanny (« IAG Nounou ») est une stratégie proposée, potentiellement sous le contrôle de l'humanité, pour empêcher la création d'une superintelligence dangereuse ainsi que pour faire face à d'autres menaces majeures au bien-être humain telles que la subversion du système financier mondial ; jusqu'à ce qu'une véritable superintelligence puisse être créée en toute sécurité. Cela implique la création d’une IAG plus intelligente que l'humain sans pour autant être superintelligente. Celle-ci serait connectée à un vaste réseau de surveillance dans le but de surveiller l’humanité et de la protéger du danger. La réglementation de potentielles IAs conscientes se concentre sur la façon de les intégrer dans la société humaine existante et peut être divisée en considérations sur leur statut juridique et sur leurs droits moraux. La réglementation de l’IA a été considérée comme restrictive, risquant d’empêcher le développement de l’IAG. Toutefois, le juriste Bertrand Warusfel souligne que les enjeux premiers et majeurs de calculabilité et d'automatisation sont absents de ces régulations.

Orientation mondiale
Contexte
La création d’un conseil de gouvernance mondial pour réglementer le développement de l’IA a été suggérée au moins dès 2017. En décembre 2018, le Canada et la France ont annoncé leur intention de créer, sur le modèle du GIEC, un groupe international d'experts sur l'intelligence artificielle soutenu par le G7. Son but serait d'étudier les effets mondiaux de l'IA sur les gens et l'économie et d'orienter le développement de l'IA. En 2019, le Groupe a été rebaptisé Partenariat mondial sur l’intelligence artificielle.
Les principes sur l'IA de l'OCDE ont été adoptées en mai 2019 et les principes sur l'IA du G20 en juin 2019. En septembre 2019, le Forum économique mondial a publié dix AI Government Procurement Guidelines (« Lignes directrices relatives à l’approvisionnement en IA au sein des gouvernements »). En février 2020, l’Union européenne a publié son document de stratégie pour promouvoir et réglementer l’IA.

Organisations internationales
Le Partenariat mondial sur l'intelligence artificielle (PMIA) a été lancé en juin 2020, affirmant la nécessité de développer l'IA conformément aux droits de l'homme et aux valeurs démocratiques, afin de garantir la confiance du public dans cette technologie, comme indiqué dans les Principes sur l'IA adoptés par l'OCDE en 2019. Les 15 membres fondateurs du PMIA sont: Australie, Canada, Union européenne, France, Allemagne, Inde, Italie, Japon, République de Corée, Mexique, Nouvelle-Zélande, Singapour, Slovénie, États-Unis et Royaume-Uni. Le Secrétariat du PMIA est hébergé par l'OCDE à Paris. Le mandat du PMIA couvre quatre thématiques. Le Centre d'expertise International de Montréal en intelligence artificielle (CEIMIA) s'occupe des deux thèmes IA responsable et gouvernance des données. Le thème avenir du travail et le thème innovation et commercialisation sont quant à eux traités à Paris. Le PMIA a aussi étudié comment l’IA peut être exploitée pour répondre à la pandémie de Covid-19.
À l'ONU, plusieurs entités ont commencé à promouvoir et à discuter des aspects de la réglementation et de la politique de l'IA, notamment le Centre pour l'IA et la robotique de l'UNICRI. En partenariat avec INTERPOL, le Centre pour l'IA et la robotique a publié le rapport AI and Robotics for Law Enforcement (« IA et robotique pour l'application de la loi ») en avril 2019 et le rapport de suivi Towards Responsible AI Innovation (« Vers une innovation responsable en IA ») en mai 2020. Lors de la 40e session scientifique de l'UNESCO en novembre 2019, l'organisation a lancé un processus de deux ans visant à parvenir à un « instrument normatif mondial sur l'éthique de l'intelligence artificielle ». Dans la poursuite de cet objectif, des forums et des conférences de l'UNESCO sur l'IA ont été organisées pour recueillir les points de vue des parties prenantes. Un projet de texte Recommendation on the Ethics of AI (« Recommandations sur l'éthique de l'IA ») du groupe d'experts ad hoc de l'UNESCO a été publié en septembre 2020. Il incluait un appel à combler des lacunes législatives. L'UNESCO l'a présenté pour adoption lors de sa Conférence générale en novembre 2021, et il a été adopté. Alors que l’ONU progresse dans la gestion mondiale de l’IA, sa capacité institutionnelle et juridique à gérer le risque existentiel lié à l’IA est plus limitée.
AI for Good (« l'IA pour le bien ») est une initiative de l'Union internationale des télécommunications (UIT) en partenariat avec 40 agences sœurs de l'ONU. AI for Good vise à identifier les applications pratiques de l'IA pour faire progresser les objectifs de développement durable de l'ONU et à faire évoluer ces solutions pour un impact mondial. Il s'agit d'une plateforme de l'ONU orientée vers l'action, mondiale et inclusive, qui favorise le développement de l'IA afin d'avoir un impact positif. Notamment sur la santé, le climat, l'égalité des genres, la prospérité inclusive, les infrastructures durables et d'autres priorités de développement mondial.

Convention internationale
En 2024, le Conseil de l’Europe adopte le premier traité international portant des normes juridiques relatives aux droits de l’homme, de démocratie et d’État de droit dans le cadre du recours aux systèmes d’intelligence artificielle (IA), sous la dénomination « convention-cadre du Conseil de l’Europe sur l’intelligence artificielle et les droits de l’homme, la démocratie et l’État de droit ».
Dans l'Union européenne, cette convention internationale est critiquée par le Contrôleur européen de la protection des données qui considère que ce traité a été édulcoré sous la pression d’États étrangers, pour ne pas s'appliquer aux entreprises privées et pour ne pas s'appliquer à certaines services des États.

Normes
L’Organisation internationale de normalisation (ISO) crée en 2017 un comité ISO/IEC JTC 1/SC 42 pour traiter des questions de normalisation dans le domaine de l’intelligence artificielle. Son secrétariat se trouve à l'American National Standards Institute (ANSI) à Washington DC aux États-Unis. Il fournit des orientations à l'organe de référence mondial pour la normalisation des technologies de l'information (JTC 1), à la Commission électrotechnique internationale (IEC) et à l'ISO sur le développement des applications fondées sur l’intelligence artificielle.
En octobre 2024, la commission publie 31 normes ; 37 autres sont en projet. Les normes ISO sur l'intelligence artificielle, d'application volontaire, portent sur des sujets divers comme la qualité des données pour les analyses de données et l’apprentissage automatique (ISO/IEC 5259), l'évaluation des performances de classification de l'apprentissage machine (ISO/IEC TS 4213), le processus de cycle de vie des systèmes d'IA (ISO/IEC 5338), la contrôlabilité des systèmes d'intelligence artificiels automatisés (ISO/IEC TS 8200), l' architecture de référence des big data (ISO/IEC TR 20547), les implications de gouvernance de l'utilisation par des organisations de l'intelligence artificielle (ISO/IEC 38507). La norme ISO/IEC 42001 spécifie les exigences et permet une utilisation responsable des systèmes de management de l’intelligence artificielle (SMIA).

Réglementation régionale et nationale
Le paysage réglementaire et politique de l’IA est un problème émergent dans les juridictions du monde entier. Depuis début 2016, de nombreuses autorités nationales, régionales et internationales ont commencé à adopter des stratégies, des plans d’action et des documents d'orientation politique sur l’IA. Ces documents couvrent un large éventail de sujets tels que la réglementation, la gouvernance, la stratégie industrielle, la recherche, les talents et les infrastructures.
Les pays ont abordé le problème de différentes manières. Concernant les trois plus grandes économies, il a été dit que « les États-Unis suivent une approche basée sur le marché, la Chine met en avant une approche basée sur l'État et l'Union Européenne poursuit une approche basée sur les droits ».

Canada
La Stratégie pancanadienne en matière d'intelligence artificielle (2017) est soutenue par un financement fédéral de 125 millions de dollars canadiens. Elle a pour objectifs d'augmenter au Canada le nombre de diplômés qualifiés et de chercheurs exceptionnels en IA, d'établir des nœuds d'excellence scientifique dans les trois principaux centres d'IA et de devenir un leader mondial sur la question des implications économiques, éthiques, politiques et juridiques des progrès de l’IA. Le programme Chaires en IA Canada-CIFAR est un élément essentiel de cette stratégie. Il bénéficie d’un financement de 86,5 millions de dollars canadiens sur cinq ans pour attirer et retenir des chercheurs de renommée mondiale en IA. Le gouvernement fédéral a nommé un Conseil consultatif en matière d'intelligence artificielle en mai 2019, chargé d'examiner comment tirer parti des atouts du Canada pour garantir que les progrès de l'IA reflètent les valeurs canadiennes, telles que les droits humains, la transparence et le logiciel libre. Le Conseil consultatif sur l'IA a créé un Groupe de travail sur la commercialisation pour en tirer une valeur économique. En 2020, le gouvernement fédéral et le gouvernement du Québec ont annoncé l'ouverture à Montréal du Centre d'expertise International de Montréal en intelligence artificielle (CEIMIA), visant à faire avancer le développement responsable de l'IA. En 2022, le gouvernement fédéral canadien a déposé le projet Loi sur l'intelligence artificielle et les données En juin 2022 démarre la seconde phase de la Stratégie pancanadienne en matière d'intelligence artificielle. En novembre 2022, le Canada a présenté la Loi de 2022 sur la mise en œuvre de la Charte du numérique (projet de loi C-27), qui propose trois lois décrites comme un ensemble holistique de législations sur la confiance et le respect de la vie privée : la « Loi sur la protection de la vie privée des consommateurs », la « Loi sur le Tribunal de la protection des renseignements personnels et des données » et la « Loi sur l’intelligence artificielle et les données ». En novembre 2023, le Canada envisage de voter un Artificial Intelligence and Data Act (AIDA).

Chine
La réglementation de l'IA en Chine est principalement régie par le « Plan de développement de l'intelligence artificielle de nouvelle génération » du Conseil des affaires de l'État de la république populaire de Chine du 8 juillet 2017 (document no 35 du Conseil d'État), dans lequel le Comité central du Parti communiste chinois et le Conseil d'État de la RPC ont exhorté les instances dirigeantes chinoises à promouvoir le développement de l'IA jusqu'en 2030. La réglementation des questions de support éthique et juridique au développement de l'IA s'accélère. La politique chinoise garantit le contrôle de l'État sur les entreprises chinoises et sur les données de valeur. Notamment sur le stockage des données sur les utilisateurs chinois dans le pays et le respect des standards nationaux sur l'IA en big data, cloud computing et logiciel industriel. En 2021, la Chine a publié des directives éthiques sur l’utilisation de l’IA en Chine, qui stipulent que les chercheurs doivent veiller à ce que l’IA respecte des valeurs humaines communes, reste sous contrôle humain et ne mette pas en danger la sécurité publique. En 2023, la Chine a introduit des mesures provisoires pour la gestion des services d'IA générative.

Conseil de l'Europe
Le Conseil de l'Europe est une organisation internationale qui promeut la démocratie, les droits de l'homme et l'État de droit. Il comprend 47 États membres, dont les 29 signataires de la Déclaration de Coopération sur l'Intelligence Artificielle faite en 2018 par l'Union européenne. Le Conseil de l'Europe a créé un espace juridique commun dans lequel ses membres ont l'obligation légale de garantir les droits énoncés dans la Convention européenne des droits de l'homme. En ce qui concerne spécifiquement l'IA, « l'objectif du Conseil de l'Europe est d'identifier les domaines d'intersection entre l'IA et nos normes en matière de droits de l'homme, de démocratie et d'État de droit, et d'élaborer des solutions pertinentes d'établissement de normes ou de renforcement des capacités ». Le conseil de l'Europe s'appuie sur divers documents tels que des lignes directrices, des chartes, des documents, des rapports et des stratégies. Ces documents proviennent de divers acteurs de la société tels que des organisations, des entreprises et des pays.

Union européenne
L'Union Européenne est l’une des plus grandes juridictions au monde et joue un rôle actif dans la réglementation mondiale des marchés numériques.
La plupart des pays de l’Union européenne (UE) ont leurs propres stratégies nationales de réglementation de l’IA, mais celles-ci sont largement convergentes. L'Union européenne est guidée par une stratégie européenne sur l'intelligence artificielle, soutenue par un groupe d'experts de haut niveau sur l'intelligence artificielle, qui mène notamment des travaux sur l'IA digne de confiance. La Commission européenne a publié ses Lignes directrices en matière d’éthique pour une IA digne de confiance en avril 2019, suivies de ses Recommandations de politique et d'investissement pour une IA de confiance en juin 2019. La Commission a aussi publié des rapports sur les aspects de sécurité et de responsabilité de l'IA, et sur l'éthique des véhicules autonomes. En 2020, la Commission européenne a sollicité des avis sur une proposition de législation spécifique à l’IA.
En février 2020, la Commission européenne a publié son Livre blanc sur l'intelligence artificielle – Une approche européenne axée sur l’excellence et la confiance. Le Livre blanc se compose de deux éléments principaux : un « écosystème d'excellence » et un « écosystème de confiance ». L'« écosystème de confiance » décrit l'approche de l'UE en matière de cadre réglementaire pour l'IA. Dans l'approche qu'elle propose, la Commission fait une distinction entre les applications d'IA « à haut risque » et celles qui ne le sont pas. Seuls les applications « à haut risque » devraient entrer dans le champ d’application d’un futur cadre réglementaire européen. Une application est considérée à haut risque si le secteur dans lequel elle est utilisée est à risque (par exemple le secteur des transports ou celui des soins de santé), et que l'utilisation qui en est fait est susceptible de faire apparaître ces risques. Les principales exigences pour les applications d'IA à haut risque concernent : « données d’entraînement » des modèle d'IA, « stockage des données », « informations à fournir », « robustesse et précision », « contrôle humain ». Il y a aussi des exigences particulières pour certains usages spécifiques tels que la biométrie. Pour les applications d'IA qui ne sont pas considérées comme « à haut risque » un système de labels est proposé sur la base du volontariat. En ce qui concerne la conformité et la mise en vigueur, la Commission envisage des évaluations préalables de la conformité, qui pourraient inclure des « procédures d'essai, d'inspection ou de certification » et/ou une « vérification des algorithmes et des ensembles de données utilisés lors de la phase de développement ». Un cadre de coopération des autorités nationales compétentes pourrait faciliter l'implémentation de cette gouvernance européenne de l'IA. En avril 2021 est officiellement proposée la législation sur l’intelligence artificielle (en anglais Artificial Intelligence Act, ou AI Act). Elle comprend un peaufinage de l’approche du Livre blanc sur l'intelligence artificielle de 2020, mais contient cette fois 4 catégories de risque : « minimal », « limité », « élevé », et « inacceptable ». Cette première version avait été critiquée notamment du fait de divers éléments flous (dont la définition large de ce qui constitue l'IA), faisant craindre des implications juridiques involontaires.
Des observateurs ont exprimé leurs inquiétudes quant à la multiplication des propositions législatives sous la Commission von der Leyen. La rapidité des initiatives législatives est en partie motivée par les ambitions politiques de l’Union européenne. Mais le nombre de lois pourrait rendre difficile le maintien de la cohérence entre elles. Parmi les principes directeurs énoncés dans les diverses propositions législatives de la Commission von der Leyen en IA figurent les objectifs d'autonomie stratégique et le concept de souveraineté numérique.
En 2023, les colégislateurs sont parvenus à un accord sur la réglementation de l'IA qui doit être suivi d'un travail technique pour finaliser le texte. L'accord prévoit par exemple des obligations de prévoir un contrôle humain sur la machine, l'établissement d'une documentation technique, ou un système de gestion du risque. Elle prévoit aussi l'information de l'utilisateur de la présence d'une intelligence artificielle.

Allemagne
En novembre 2020, l'Institut allemand de normalisation et le ministère fédéral de l'Économie et de la Protection du Climat ont publié la première édition de la « Feuille de route allemande de normalisation de l'intelligence artificielle » (« NRM KI ») et l'ont présentée au public lors du sommet numérique du gouvernement fédéral d'Allemagne. NRM KI décrit les exigences des futures réglementations et normes dans le contexte de l'IA. La mise en œuvre des recommandations vise à renforcer l'économie et la science allemandes en IA, et à créer des conditions favorables à l'innovation pour cette technologie émergente. La première édition est un document de 200 pages rédigé par 300 experts. La deuxième édition du NRM KI a été publiée à l'occasion du Sommet numérique du gouvernement allemand le 9 décembre 2022. L'Institut allemand de normalisation a coordonné plus de 570 experts participants issus d'un large éventail de domaines, dont la science, l'industrie, la société civile ou le secteur public. La deuxième édition est un document de 450 pages.
D'une part, NRM KI couvre les thèmes centraux en termes d'applications (par exemple médecine, mobilité, énergie et environnement, services financiers, automatisation industrielle) et de questions fondamentales (par exemple classification de l'IA, sécurité, certification, systèmes socio-techniques, éthique). D'autre part, il fournit un aperçu des thèmes centraux en IA et de son environnement à travers un large éventail de groupes d'intérêt et de sources d'information. Au total, le document couvre 116 besoins en matière de normalisation et fournit six principales recommandations d'action.

Royaume-Uni
Le Royaume-Uni a soutenu l’application et le développement de l’IA dans les entreprises via la Digital Economy Strategy (« Stratégie pour une économie numérique ») introduite début 2015 par Innovate UK. Dans le secteur public, des lignes directrices sur l'éthique des données ont été fournies par le Département de la Culture, des Médias et du Sport. L'Institut Alan Turing a conseillé l'implémentation et la conception responsable des systèmes d'IA. En termes de cybersécurité, le National Cyber Security Centre a publié en 2020 des lignes directrices sur les « outils de sécurité intelligents ». L’année suivante, le Royaume-Uni a publié sa stratégie nationale sur 10 ans en matière d’IA, qui décrit les actions visant à évaluer les risques à long terme liés à l’IA, y compris les risques catastrophiques liés à l’IA générale.
En mars 2023, le Royaume-Uni publie un livre blanc intitulé A pro-innovation approach to AI regulation (« Une approche de la réglementation de l'IA favorable à l'innovation »). Celui-ci présente des principes généraux, mais laisse une flexibilité importante aux régulateurs existants quant à la façon de les adapter pour les différents secteurs, tels que l'aéronautique ou les marchés financiers. En novembre 2023, le Royaume-Uni accueille le premier sommet en sûreté de l'IA, le premier ministre anglais Rishi Sunak souhaitant faire du Royaume-Uni un leader dans la réglementation en sûreté de l'IA.

États-Unis
Les discussions sur la réglementation de l’IA aux États-Unis ont porté sur des sujets tels que la question du bon moment pour réglementer l’IA, la nature du cadre réglementaire fédéral pour régir et promouvoir l’IA, la question de quelle agence devrait diriger, les pouvoirs de réglementation et de gouvernance de cette agence, comment mettre à jour les réglementations face à l’évolution rapide de la technologie, et quel rôle attribuer aux gouvernements et tribunaux des États.
Dès 2016, l’administration Obama avait commencé à s'intéresser aux risques et à la réglementation liés à l’intelligence artificielle. Dans un rapport intitulé Preparing For the Future of Artificial Intelligence (« Se préparer pour le futur de l'intelligence artificielle »), le Conseil national des sciences et technologies a créé un précédent en permettant aux chercheurs de continuer à développer de nouvelles technologies d'IA avec peu de restrictions. Il est indiqué dans le rapport que « l'approche en matière de réglementation des produits rendus possibles par l'IA qui est de protéger la sécurité publique devrait être éclairée par une évaluation des aspects du risque... ». Ces risques seraient la principale raison de créer de nouvelles réglementations, si les réglementations existantes ne s'appliquent pas à l'IA.
Le premier rapport principal était le National Strategic Research and Development Plan for Artificial Intelligence (« Plan national de recherche stratégique et de développement de l'intelligence artificielle ») Le 13 août 2018, l'article 1051 de la John S. McCain National Defense Authorization Act (« Loi John S. McCain sur l'Autorisation de Défense Nationale »)  pour l'année fiscale 2019 (PL 115-232) a créé la National Security Commission on Artificial Intelligence (« Commission de Sécurité Nationale sur l'Intelligence Artificielle »), abrégée NSCAI. Elle a notamment pour but d'« examiner les méthodes et les moyens nécessaires pour faire progresser le développement de l'intelligence artificielle, l'apprentissage automatique, et des technologies associées afin gérer de manière globale les besoins de défense et de sécurité nationale des États-Unis. » Le pilotage de la réglementation de l’IA liée à la sécurité est assuré par la NSCAI. L'Artificial Intelligence Initiative Act (« Loi d'Initiative en Intelligence Artificielle ») (S.1558) est un projet de loi qui établirait une initiative fédérale conçue pour accélérer la recherche et le développement de l'IA, entre autres pour la sécurité économique et nationale des États-Unis.
Le 7 janvier 2019, à la suite d'un décret visant à maintenir le leadership américain en matière d'intelligence artificielle, l'OSTP (Bureau de la politique scientifique et technologique) de la Maison-Blanche a publié l'ébauche de document Guidance for Regulation of Artificial Intelligence Applications (« Lignes directrices pour les applications d'intelligence artificielle »), qui comprend dix principes à l'intention des agences américaines lorsqu'elles doivent décider s'il faut réglementer l'IA et comment. En réponse, le NIST a publié un document de position, et le Defence Innovation Board a publié des recommandations sur l’utilisation éthique de l’IA.
La FDA a aussi travaillé sur la réglementation de l'IA, notamment dans le cadre de l'imagerie médicale. Le NIST a également publié son National Artificial Intelligence Research and Development Strategic Plan (« Plan national de stratégie de recherche et développement en intelligence artificielle »). Le public a pu examiner ce plan de stratégie et fournir des recommandations, afin de se rapprocher davantage d'une IA digne de confiance.
En mars 2021, la NSCAI a publié son rapport final. Ils y ont notamment écrit que les progrès en l'IA vont probablement permettre de nouvelles capacités et applications, potentiellement drastiques. Et que cela pourrait amener de nouvelles problématiques et de nouveaux risques, nécessitant ainsi de nouvelles recommandations, réglementations et avancées techniques pour s'assurer que les systèmes d'IA restent sûrs, robustes, et dignes de confiance. Le NSCAI recommande de scruter les progrès en IA, de faire les investissements nécessaires et d'y prêter une attention politique, afin d'assurer que les systèmes d'IA restent alignés avec les objectifs et les valeurs des États-Unis.
En juin 2022, les sénateurs Rob Portman et Gary Peters ont présenté le Global Catastrophic Risk Mitigation Act (« Loi sur l'atténuation des risques de catastrophe planétaire »). Le projet de loi bipartisan « aiderait également à contrer le risque que l’intelligence artificielle… soit utilisée à mauvais escient d’une manière qui pourrait présenter un risque catastrophique ». Le 4 octobre 2022, le président Joe Biden a dévoilé une nouveau AI Bill of Rights (« Déclaration de Droits liés à l'IA »), qui décrit cinq protections dont les Américains devraient bénéficier à l'ère de l'IA : des systèmes sûrs et efficaces, une protection contre la discrimination algorithmique, la confidentialité des données, des explications, et enfin des alternatives et considérations humaines. Le projet de loi a été présenté en octobre 2021 par l'OSTP.
En janvier 2023, la New York City Bias Audit Law (« Loi de New York City sur l'audit de biais ») a été promulguée par le Conseil de New York en novembre 2021. Elle est entrée en application le 5 Juillet 2023 et elle interdit aux entreprises d'utiliser des outils automatisés de décision pour embaucher des candidats ou promouvoir des employés, à moins que les outils n'aient été audités de manière indépendante à la recherche de biais.
En juillet 2023, l’administration Biden-Harris a obtenu des engagements volontaires de la part de sept entreprises : Amazon, Anthropic, Google, Inflection AI, Meta, Microsoft et OpenAI. Le but est de gérer les risques associés à l’IA. Ces entreprises se sont engagées à :

Garantir que les produits d’IA soient soumis à des tests internes et externes de sécurité avant leur diffusion publique.
Partager des informations sur la gestion des risques liés à l’IA avec l’industrie, les gouvernements, la société civile et le monde universitaire.
Prioriser la cybersécurité et la protection d'informations concernant leurs systèmes d'IA.
Développer des mécanismes pour informer les utilisateurs lorsqu'un contenu est généré par l’IA.
Faire connaître publiquement les capacités, limites et utilisations de leurs systèmes d'IA.
Prioriser la recherche sur les risques sociétaux posés par l’IA (notamment les biais, la discrimination et les problèmes de confidentialité des données).
Développer des systèmes d'IA pour adresser des défis sociétaux tels que la prévention du cancer ou l'atténuation du changement climatique.
En septembre 2023, huit entreprises supplémentaires ont souscrit à ces engagements volontaires : Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI et Stability AI.

Brésil
Le 30 septembre 2021, la Chambre des députés brésilienne a approuvé le Marco Legal da Inteligência Artificial (« Cadre juridique pour l'intelligence artificielle ») dans le cadre des efforts de réglementation pour le développement et l'utilisation des technologies d'IA. Cette loi vise à stimuler davantage la recherche et l'innovation dans les solutions d'IA visant l'éthique, la culture, la justice, l’équité et l'imputabilité. Il contient 10 articles, qui traitent des objectifs, de l'élaboration de principes éthiques, de la promotion d'investissements soutenus dans la recherche, et de l'élimination des obstacles à l'innovation. Notamment, l’article 4 met l’accent sur le fait d'éviter des solutions discriminatoires en matière d’IA, sur la pluralité et sur le respect des droits de l’homme. En outre, cette loi insiste sur l'importance du principe d'égalité dans les algorithmes de prise de décision, en particulier pour les sociétés très diverses et multiethniques comme celle du Brésil.
Lorsque le projet de loi a été rendu public, il a fait l’objet de nombreuses critiques, ses détracteurs lui reprochant de ne pas traiter adéquatement de l'imputabilité, la transparence et l’inclusivité. L'article VI établit la responsabilité subjective, ce qui signifie que toute personne endommagée par un système d'IA et souhaitant recevoir une compensation doit préciser la partie prenante et prouver qu'il y a eu une erreur dans le cycle de vie de la machine. Des chercheurs estiment qu’il est contraire à l’ordre juridique de qu'un individu soit chargé de prouver une erreur algorithmique étant donné le degré élevé d’autonomie, d’imprévisibilité et de complexité des systèmes d’IA. Cela a également attiré l'attention sur les problèmes actuels des systèmes de reconnaissance faciale au Brésil qui sont parfois accusés de conduire à des arrestations injustes par la police. Selon ses détracteurs, cette loi implique que les individus se retrouveraient à devoir prouver que la machine a commis une erreur.
La principale controverse autour de ce projet de loi concernait la mise en application des trois principes proposés. Premièrement, le principe de non-discrimination suggère que l’IA doit être développée et utilisée de manière à atténuer la possibilité de pratiques abusives et discriminatoires. Deuxièmement, la poursuite du principe de neutralité énumère des recommandations destinées aux parties prenantes afin d'atténuer les préjugés. Enfin, le principe de transparence stipule que la transparence d'un système est nécessaire seulement lorsqu'il existe un risque élevé de violation des droits fondamentaux. La loi donne des lignes directrices souples, et il lui a été reproché de manquer de clauses obligatoires et contraignantes. Il lui a aussi été reproché de ne pas avoir fait intervenir suffisamment de parties prenantes.

Réglementation des armes entièrement autonomes
Les questions juridiques liées aux systèmes d'armes létales autonomes, notamment le respect du droit international humanitaire, sont discutées à l'ONU depuis 2013, dans le cadre de la Convention sur certaines armes classiques. Des réunions informelles d’experts ont notamment eu lieu en 2014, 2015 et 2016 et un groupe d’experts gouvernementaux a été nommé pour poursuivre les délibérations sur la question en 2016. Un ensemble de principes sur ces armes, mis en avant par le groupe d'experts gouvernementaux, a été adopté en 2018.
En 2016, la Chine a publié un document de position remettant en question l’adéquation du droit international existant pour faire face à l’éventualité d’armes entièrement autonomes, devenant ainsi le premier membre permanent du Conseil de sécurité des Nations unies à aborder la question, ce qui a débouché sur des propositions de réglementation mondiale. La possibilité d'un moratoire ou d'une interdiction préventive du développement et de l'utilisation d'armes létales autonomes a également été soulevée à plusieurs reprises par d'autres délégations nationales à la Convention sur certaines armes classiques. Une telle interdiction est aussi fortement défendue par la campagne Stop Killer Robots (« Arrêtez les robots tueurs »), une coalition internationale d'organisations non gouvernementales. Le gouvernement américain soutient que le droit international humanitaire actuel est capable de réglementer le développement ou l’utilisation des armes létales autonomes. Le Service de recherche du Congrès a indiqué en 2023 que les États-Unis n'ont pas d'armes létales autonomes dans son inventaire, mais que sa politique n'en interdit pas le développement et l'utilisation.

Voir aussi
Bibliographie
Par ordre alphabétique :

Alexandra Bensamoun, Grégoire Loiseau, Droit de l'intelligence artificielle, LGDJ, 29 novembre 2022, 600 p.
Céline Castets-Renard, Jessica Eynard, Un droit de l'intelligence artificielle : entre règles sectorielles et régime général ; Perspectives comparées, Bruylant, 16 mars 2023, 996 p.
Sabine Marcelin, Droit de l'intelligence artificielle, Gualino, mars 2025, 192 p.
Godefroy Lêmy, Droit de l'intelligence artificielle, Ellipses, 8 juillet 2025, 352 p.

Articles connexes
Intelligence artificielle
Règlement sur l'intelligence artificielle (Union européenne)
Alignement de l'IA
Éthique de l'intelligence artificielle
Régulation des algorithmes

Liens externes
Recommandation sur l'éthique de l'intelligence artificielle, UNESCO, adoptée le 23 novembre 2021 par les 193 États-membres des Nations unies, comportant quatre valeurs, dix principes et 11 domaines d'action stratégiques

Références
(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Regulation of artificial intelligence » (voir la liste des auteurs).

 Portail de l’intelligence artificielle   Portail du droit   Portail des technologies   Portail des relations internationales