import numpy as np
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer


class Vectorizor:
    def __init__(
        self,
        model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    ):
        """
        Initialise le vectorizor avec un modÃ¨le par dÃ©faut.

        Args:
            model_name (str): Nom du modÃ¨le HuggingFace Ã  charger
        """
        self.model_name = model_name
        self.model = None
        self._model_cache = {}  # Cache des modÃ¨les chargÃ©s
        self._load_model(model_name)
        return

    def _load_model(self, model_name: str):
        """
        Charge un modÃ¨le d'embedding avec gestion d'erreurs robuste.

        Args:
            model_name (str): Nom du modÃ¨le HuggingFace
        """

        # VÃ©rifier le cache d'abord
        if model_name in self._model_cache:
            print(f"âš¡ ModÃ¨le rÃ©cupÃ©rÃ© du cache : {model_name}")
            self.model = self._model_cache[model_name]
            self.model_name = model_name
            return

        print(f" Chargement du modÃ¨le : {model_name}")

        try:
            # Cas spÃ©cial : Qwen nÃ©cessite trust_remote_code
            if "Qwen" in model_name or "qwen" in model_name:
                try:
                    model = SentenceTransformer(
                        model_name,
                        tokenizer_kwargs={"padding_side": "left"},
                        device="cuda",
                        trust_remote_code=True,  #  CRUCIAL pour Qwen3
                    )
                except Exception as qwen_error:
                    print(f"  Impossible de charger Qwen : {qwen_error}")
                    print("   â†’ Fallback vers MPNet")

                    # Fallback vers MPNet
                    fallback_model = (
                        "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
                    )
                    model = SentenceTransformer(fallback_model, device="cuda")
                    model_name = fallback_model  # Mettre Ã  jour le nom
                    print(f" Fallback rÃ©ussi : {fallback_model}")

                    import torch
                    print(f"ğŸ® GPU disponible : {torch.cuda.is_available()}")
                    print(f"ğŸ® GPU utilisÃ© : {next(model.parameters()).device}")


            else:
                # Autres modÃ¨les (BERT, MPNet, MiniLM, etc.)
                model = SentenceTransformer(model_name, device="cuda")

            # Stocker dans le cache
            self._model_cache[model_name] = model
            self.model = model
            self.model_name = model_name

            print(f" ModÃ¨le chargÃ© : {model_name}")

            import torch
            print(f"ğŸ® GPU disponible : {torch.cuda.is_available()}")
            print(f"ğŸ® GPU utilisÃ© : {next(model.parameters()).device}")


        except Exception as e:
            print(f" ERREUR CRITIQUE lors du chargement de {model_name} : {e}")

            # Dernier fallback : MPNet par dÃ©faut
            fallback_model = (
                "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
            )

            if model_name != fallback_model:
                print(f"   â†’ Fallback final vers {fallback_model}")
                try:
                    model = SentenceTransformer(fallback_model, device="cuda")
                    self._model_cache[fallback_model] = model
                    self.model = model
                    self.model_name = fallback_model
                    print(f" Fallback final rÃ©ussi")
                except Exception as final_error:
                    print(f" FALLBACK Ã‰CHOUÃ‰ : {final_error}")
                    raise RuntimeError("Impossible de charger un modÃ¨le d'embedding")
            else:
                raise

    def switch_to_model_for_collection(self, collection_metadata: dict):
        """
         SWITCH DYNAMIQUE : Change le modÃ¨le selon les mÃ©tadonnÃ©es de la collection.
        ROBUSTE : GÃ¨re les mÃ©tadonnÃ©es manquantes et les modÃ¨les non chargeables.

        Args:
            collection_metadata (dict): MÃ©tadonnÃ©es de la collection ChromaDB
        """

        # RÃ©cupÃ©rer le nom du modÃ¨le depuis les mÃ©tadonnÃ©es
        original_model = collection_metadata.get("model", "unknown")

        #  MAPPING : Nom court â†’ Chemin HuggingFace complet
        model_map = {
            # Qwen
            "Qwen3-Embedding-0.6B": "Qwen/Qwen3-Embedding-0.6B",
            "Qwen/Qwen3-Embedding-0.6B": "Qwen/Qwen3-Embedding-0.6B",
            # MiniLM
            "paraphrase-multilingual-MiniLM-L12-v2": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            # MPNet
            "paraphrase-multilingual-mpnet-base-v2": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            #  NOUVEAU : Gestion des mÃ©tadonnÃ©es manquantes
            "unknown": None,  # Pas de switch si inconnu
            "N/A": None,
        }

        required_model = model_map.get(original_model)

        #  CAS 1 : MÃ©tadonnÃ©es manquantes ou invalides
        if required_model is None:
            print(f"\n  MÃ©tadonnÃ©es manquantes (model={original_model})")
            print(f"   â†’ Conservation du modÃ¨le actuel : {self.model_name}")
            print(f"    Conseil : RecrÃ©ez cette collection avec manage_collections.py")
            return  # Ne rien changer

        #  CAS 2 : ModÃ¨le identique, pas de rechargement
        if required_model == self.model_name:
            return  # Rien Ã  faire

        #  CAS 3 : Changement nÃ©cessaire
        print(f"\n Changement de modÃ¨le dÃ©tectÃ© :")
        print(f"   Actuel : {self.model_name}")
        print(f"   Requis : {required_model}")

        # Recharger (avec gestion d'erreurs intÃ©grÃ©e)
        self._load_model(required_model)

    def get_model_dimension(self) -> int:
        """
        Retourne la dimension du modÃ¨le actuel.

        Returns:
            int: Dimension des embeddings (ex: 768, 384, 1024)
        """
        test_emb = self.model.encode("test")
        return len(test_emb)

    def vectorize(self):
        """MÃ©thode legacy (non utilisÃ©e)"""
        return

    def encode(self, text_series: pd.Series) -> pd.Series:
        """
        Encode une sÃ©rie de textes en embeddings.

        Args:
            text_series (pd.Series): SÃ©rie de textes Ã  encoder

        Returns:
            pd.Series: SÃ©rie d'embeddings (numpy arrays)
        """
        tqdm.pandas(desc="GÃ©nÃ©ration des embeddings")
        embeddings = text_series.progress_apply(lambda x: self.model.encode(x))
        return embeddings

    def encode_query(self, query: str) -> np.ndarray:
        """
        Encode une requÃªte unique.

        Args:
            query (str): RequÃªte Ã  encoder

        Returns:
            np.ndarray: Embedding de la requÃªte
        """
        # Certains modÃ¨les supportent prompt_name, d'autres non
        try:
            query_embeddings = self.model.encode(query, prompt_name="query").astype(
                float
            )
        except (TypeError, AttributeError):
            query_embeddings = self.model.encode(query).astype(float)

        return query_embeddings

    def similarity(self, target_embeddings, db_embeddings):
        """
        Calcule la similaritÃ© entre embeddings.

        Args:
            target_embeddings: Embedding cible
            db_embeddings: Embeddings de la base

        Returns:
            Scores de similaritÃ©
        """
        return self.model.similarity(target_embeddings, db_embeddings)
