import sys
from pathlib import Path
from src.rag import Retrieval
import time
from typing import List, Dict, Any

ROOT_DIR = Path(__file__).parent.parent  # Remonte de launchers/ √† RAG_CGT/
sys.path.insert(0, str(ROOT_DIR))


def get_collection_metadata(collection) -> Dict[str, Any]:
    """
    R√©cup√®re les m√©tadonn√©es directement depuis ChromaDB.

    Args:
        collection: Objet collection ChromaDB

    Returns:
        Dict: M√©tadonn√©es compl√®tes ou valeurs par d√©faut
    """
    try:
        metadata = collection.metadata

        return {
            "chunk_size": metadata.get("chunk_size"),
            "overlap": metadata.get("overlap"),
            "source_folder": metadata.get("source_folder"),
            "created_at": metadata.get("created_at"),
            "model": metadata.get("model", "N/A"),
            "reranking_enabled": metadata.get("reranking_enabled"),
            "created_by": metadata.get("created_by", "N/A"),
            "version": metadata.get("version", "N/A"),
        }
    except Exception as e:
        print(f"Impossible de lire les m√©tadonn√©es : {e}")
        return {
            "chunk_size": None,
            "overlap": None,
            "source_folder": None,
            "created_at": None,
            "model": "N/A",
            "reranking_enabled": None,
            "created_by": "N/A",
            "version": "N/A",
        }


def select_collections_menu() -> List[str]:
    """
    Menu interactif pour s√©lectionner les collections √† benchmarker.

    Returns:
        List[str]: Liste des noms de collections s√©lectionn√©es
    """
    r = Retrieval()
    client = r.chroma_storage.chroma_client
    collections = r.chroma_storage.list_collection_names()

    if not collections:
        print("\nAucune collection trouv√©e dans ChromaDB")
        return []

    print("\n" + "=" * 100)
    print("S√âLECTION DES COLLECTIONS √Ä BENCHMARKER")
    print("=" * 100)
    print(f"\n{len(collections)} collection(s) disponible(s) :\n")

    # Afficher chaque collection avec ses infos
    collections_info = []
    for i, col_name in enumerate(collections, 1):
        col = client.get_collection(col_name)
        count = col.count()
        metadata = get_collection_metadata(col)

        collections_info.append(
            {"num": i, "name": col_name, "count": count, "metadata": metadata}
        )

        # Affichage format√©
        print(f"   [{i}] {col_name}")
        print(f"       Documents : {count}")

        if metadata["chunk_size"]:
            overlap_str = (
                f"{metadata['overlap'] * 100:.0f}%" if metadata["overlap"] else "N/A"
            )
            print(
                f"       Param√®tres : {metadata['chunk_size']} mots, {overlap_str} overlap"
            )

        if metadata["source_folder"] and metadata["source_folder"] != "N/A":
            print(f"       Source : {metadata['source_folder']}")

        if metadata["created_by"] != "N/A":
            print(f"       Cr√©√©e par : {metadata['created_by']}")

        print()

    # Menu de s√©lection
    print("=" * 100)
    print("Options de s√©lection :")
    print("   ‚Ä¢ Tapez 'all' pour toutes les collections")
    print("   ‚Ä¢ Tapez les num√©ros s√©par√©s par des virgules (ex: 1,3,5)")
    print("   ‚Ä¢ Tapez les noms s√©par√©s par des virgules (ex: CGT, config_150_15)")

    choix = input("\nVotre choix : ").strip()

    # Traiter le choix
    if choix.lower() == "all":
        selected = collections
        print(f"\nToutes les {len(collections)} collection(s) s√©lectionn√©es")

    elif choix.replace(",", "").replace(" ", "").isdigit():
        # S√©lection par num√©ros
        nums = [int(n.strip()) for n in choix.split(",") if n.strip().isdigit()]
        selected = []

        for num in nums:
            if 1 <= num <= len(collections):
                selected.append(collections[num - 1])
            else:
                print(f"Num√©ro {num} hors limite (ignor√©)")

        print(
            f"\n{len(selected)} collection(s) s√©lectionn√©e(s) : {', '.join(selected)}"
        )

    else:
        # S√©lection par noms
        noms = [n.strip() for n in choix.split(",")]
        selected = []

        for nom in noms:
            if nom in collections:
                selected.append(nom)
            else:
                print(f"Collection '{nom}' introuvable (ignor√©e)")

        if selected:
            print(
                f"\n{len(selected)} collection(s) s√©lectionn√©e(s) : {', '.join(selected)}"
            )
        else:
            print("\nAucune collection valide s√©lectionn√©e")

    return selected


def benchmark_collections(
    collections_to_test: List[str], query: str, n_results: int = 5, verbose: bool = True
) -> List[Dict[str, Any]]:
    """
    Benchmark les collections sp√©cifi√©es avec UNE requ√™te.

    Args:
        collections_to_test (List[str]): Noms des collections √† tester
        query (str): Requ√™te de test
        n_results (int): Nombre de r√©sultats √† r√©cup√©rer
        verbose (bool): Afficher les d√©tails

    Returns:
        List[Dict]: R√©sultats du benchmark
    """

    if verbose:
        print("\n" + "=" * 100)
        print(f" BENCHMARK DE {len(collections_to_test)} COLLECTION(S)")
        print("=" * 100)
        print(f" Requ√™te : {query}\n")

    rag_instance = Retrieval()
    client = rag_instance.chroma_storage.chroma_client

    results = []

    for col_name in collections_to_test:
        if verbose:
            print(f"\n{'=' * 70}")
            print(f"üìä Test : {col_name}")
            print(f"{'=' * 70}")

        try:
            # Switch vers la collection
            rag_instance.chroma_storage.switch_collection(col_name)

            # R√©cup√©rer les stats
            stats = rag_instance.chroma_storage.get_stats()
            total_chunks = stats["total_documents"]

            # R√©cup√©rer m√©tadonn√©es
            collection = client.get_collection(col_name)
            params = get_collection_metadata(collection)

            if verbose:
                print(f"   ‚Ä¢ Total chunks : {total_chunks}")
                if params["chunk_size"]:
                    print(
                        f"   ‚Ä¢ Param√®tres : {params['chunk_size']} mots, {params['overlap'] * 100:.0f}% overlap"
                    )
                    if params["source_folder"]:
                        print(f"   ‚Ä¢ Source : {params['source_folder']}")

            # Skip si collection vide
            if total_chunks == 0:
                if verbose:
                    print("  Collection vide, skip")
                continue

            # Test de recherche
            start_time = time.time()
            contexts, sources, scores = rag_instance.query(query, n=n_results)
            query_time = time.time() - start_time

            result = {
                "config_name": col_name,
                "chunk_size": params["chunk_size"],
                "overlap": params["overlap"],
                "source_folder": params["source_folder"],
                "total_chunks": total_chunks,
                "total_files": stats["total_fichiers"],
                "query_time": query_time,
                "best_score": scores[0] if scores else 0,
                "avg_score": sum(scores) / len(scores) if scores else 0,
                "top_context": contexts[0][:150] + "..." if contexts else "",
                "top_source": sources[0] if sources else "N/A",
                "created_by": params.get("created_by", "N/A"),
                "created_at": params.get("created_at", "N/A"),
                "version": params.get("version", "N/A"),
                "query": query,
            }

            results.append(result)

            if verbose:
                print(f"   ‚Ä¢ Temps recherche : {query_time:.3f}s")
                print(f"   ‚Ä¢ Score top-1 : {result['best_score']:.3f}")
                print(f"   ‚Ä¢ Score moyen : {result['avg_score']:.3f}")

        except Exception as e:
            if verbose:
                print(f"   Erreur : {e}")
            continue

    return results


def benchmark_single_query(
    query: str = "Quels sont les dangers de l'intelligence artificielle ?",
    n_results: int = 5,
    collections: List[str] = None,
) -> List[Dict[str, Any]]:
    """
    Benchmark avec UNE SEULE requ√™te.

    Args:
        query (str): Requ√™te de test
        n_results (int): Nombre de r√©sultats
        collections (List[str]): Collections √† tester (None = menu de s√©lection)

    Returns:
        List[Dict]: R√©sultats du benchmark
    """

    # S√©lection des collections si non sp√©cifi√©es
    if collections is None:
        collections = select_collections_menu()

    if not collections:
        print("Aucune collection s√©lectionn√©e")
        return []

    # Lancer le benchmark
    return benchmark_collections(collections, query, n_results, verbose=True)


def benchmark_multiple_queries(
    queries: List[str] = None, n_results: int = 5, collections: List[str] = None
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Benchmark avec PLUSIEURS requ√™tes.

    Args:
        queries (List[str]): Liste des requ√™tes (None = d√©faut)
        n_results (int): Nombre de r√©sultats par requ√™te
        collections (List[str]): Collections √† tester (None = menu)

    Returns:
        Dict: R√©sultats group√©s par requ√™te
    """

    # Requ√™tes par d√©faut
    if queries is None:
        queries = [
            "Quels sont les dangers de l'intelligence artificielle ?",
            "Histoire de l'intelligence artificielle",
            "Applications pratiques de l'IA",
            "Qu'est-ce que l'apprentissage automatique ?",
            "√âthique de l'intelligence artificielle",
        ]

    # S√©lection des collections
    if collections is None:
        collections = select_collections_menu()

    if not collections:
        print("Aucune collection s√©lectionn√©e")
        return {}

    print("\n" + "=" * 100)
    print(" BENCHMARK MULTI-REQU√äTES")
    print("=" * 100)
    print(
        f"\n{len(queries)} requ√™te(s) √ó {len(collections)} collection(s) = {len(queries) * len(collections)} tests\n"
    )

    all_results = {}

    for i, query in enumerate(queries, 1):
        print(f"\n{'=' * 100}")
        print(f"REQU√äTE {i}/{len(queries)} : {query}")
        print("=" * 100)

        # Benchmark avec cette requ√™te
        results = benchmark_collections(collections, query, n_results, verbose=True)
        all_results[query] = results

    return all_results


def print_comparison_table(results: List[Dict[str, Any]], title: str = "R√âSULTATS"):
    """
    Affiche le tableau comparatif format√©.

    Args:
        results (List[Dict]): Liste des r√©sultats
        title (str): Titre du tableau
    """

    if not results:
        print("\nAucun r√©sultat √† afficher")
        return

    print("\n\n" + "=" * 110)
    print(f"=== TABLEAU COMPARATIF - {title} ===")
    print("=" * 110)

    # En-t√™te
    print(
        f"{'Collection':<25} {'Size':<8} {'Overlap':<10} {'Chunks':<10} "
        f"{'Files':<8} {'Temps (s)':<12} {'Score':<8}"
    )
    print("-" * 110)

    # Lignes
    for r in results:
        size_str = str(r["chunk_size"]) if r["chunk_size"] else "N/A"
        overlap_str = (
            f"{r['overlap'] * 100:.0f}%" if r["overlap"] is not None else "N/A"
        )

        print(
            f"{r['config_name']:<25} {size_str:<8} {overlap_str:<10} "
            f"{r['total_chunks']:<10} {r['total_files']:<8} "
            f"{r['query_time']:<12.3f} {r['best_score']:<8.3f}"
        )

    # Recommandations
    print("\n" + "=" * 110)
    print("MEILLEURES CONFIGURATIONS :")

    best_score_config = max(results, key=lambda x: x["best_score"])
    fastest_config = min(results, key=lambda x: x["query_time"])
    smallest_config = min(results, key=lambda x: x["total_chunks"])

    print(
        f"    Meilleur score : {best_score_config['config_name']} ({best_score_config['best_score']:.3f})"
    )
    print(
        f"   ‚ö° Plus rapide : {fastest_config['config_name']} ({fastest_config['query_time']:.3f}s)"
    )
    print(
        f"   Plus compacte : {smallest_config['config_name']} ({smallest_config['total_chunks']} chunks)"
    )

    # Recommandation √©quilibr√©e
    balanced = max(
        results, key=lambda x: (x["best_score"] * 0.6) + (1 - x["query_time"] / 5) * 0.4
    )
    print(f"\n RECOMMANDATION (√©quilibre score + vitesse) : {balanced['config_name']}")
    if balanced["chunk_size"]:
        print(
            f"   Param√®tres : {balanced['chunk_size']} mots, {balanced['overlap'] * 100:.0f}% overlap"
        )

    print("=" * 110)


def print_multi_query_summary(all_results: Dict[str, List[Dict[str, Any]]]):
    """
    Affiche un r√©sum√© comparatif multi-requ√™tes.

    Args:
        all_results (Dict): R√©sultats group√©s par requ√™te
    """

    print("\n\n" + "=" * 100)
    print("=== R√âSUM√â MULTI-REQU√äTES ===")
    print("=" * 100)

    # Pour chaque requ√™te, meilleure collection
    for query, results in all_results.items():
        print(f"\nRequ√™te : {query}")

        if not results:
            print("   Aucun r√©sultat")
            continue

        best = max(results, key=lambda x: x["best_score"])
        print(
            f"   Meilleure : {best['config_name']} (score: {best['best_score']:.3f}, temps: {best['query_time']:.3f}s)"
        )

    # Analyse globale
    print("\n" + "=" * 100)
    print("ANALYSE GLOBALE (moyenne sur toutes les requ√™tes)")
    print("=" * 100)

    # Scores moyens par collection
    collection_scores = {}

    for query, results in all_results.items():
        for result in results:
            col_name = result["config_name"]
            if col_name not in collection_scores:
                collection_scores[col_name] = []
            collection_scores[col_name].append(result["best_score"])

    # Moyennes
    collection_averages = {
        col: sum(scores) / len(scores) for col, scores in collection_scores.items()
    }

    # Tri d√©croissant
    sorted_collections = sorted(
        collection_averages.items(), key=lambda x: x[1], reverse=True
    )

    print("\nClassement (score moyen) :")
    for i, (col_name, avg_score) in enumerate(sorted_collections, 1):
        print(f"   {i}. {col_name} : {avg_score:.3f}")

    # Meilleure globale
    if sorted_collections:
        best_overall = sorted_collections[0]
        print(f"\nMEILLEURE COLLECTION GLOBALE : {best_overall[0]}")
        print(f"   Score moyen : {best_overall[1]:.3f}")
        print(f"   Test√© sur {len(all_results)} requ√™te(s)")

    print("=" * 100)


def export_results(
    results: List[Dict[str, Any]], filename: str = "benchmark_results.txt"
):
    """Exporte les r√©sultats mono-requ√™te"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write("=" * 100 + "\n")
        f.write("BENCHMARK CHROMADB - R√âSULTATS\n")
        f.write("=" * 100 + "\n\n")

        if results:
            f.write(f"Requ√™te : {results[0].get('query', 'N/A')}\n\n")

        for r in results:
            f.write(f"Collection : {r['config_name']}\n")
            f.write(f"   Chunk size : {r['chunk_size'] or 'N/A'}\n")
            f.write(f"   Overlap : {r['overlap'] * 100 if r['overlap'] else 'N/A'}%\n")
            f.write(f"   Score : {r['best_score']:.3f}\n")
            f.write(f"   Temps : {r['query_time']:.3f}s\n\n")

    print(f"\nR√©sultats export√©s : {filename}")


def export_multi_query_results(
    all_results: Dict[str, List[Dict[str, Any]]],
    filename: str = "benchmark_multi_queries.txt",
):
    """Exporte les r√©sultats multi-requ√™tes"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write("=" * 100 + "\n")
        f.write("BENCHMARK MULTI-REQU√äTES\n")
        f.write("=" * 100 + "\n\n")

        for i, (query, results) in enumerate(all_results.items(), 1):
            f.write(f"\nREQU√äTE {i} : {query}\n")
            f.write("-" * 100 + "\n")

            for r in results:
                f.write(f"{r['config_name']:<30} Score: {r['best_score']:.3f}\n")

            if results:
                best = max(results, key=lambda x: x["best_score"])
                f.write(f"\nMeilleure : {best['config_name']}\n\n")

    print(f"\nR√©sultats multi-requ√™tes export√©s : {filename}")


def interactive_menu():
    """Menu interactif principal"""

    print("\n" + "=" * 100)
    print("BENCHMARK COLLECTIONS CHROMADB")
    print("=" * 100)
    print("\nOptions :")
    print("   1. Benchmark avec UNE requ√™te (rapide)")
    print("   2. Benchmark avec PLUSIEURS requ√™tes par d√©faut (th√®me IA)")
    print("   3. Benchmark personnalis√©")
    print("   4. Quitter")

    choix = input("\nVotre choix (1-4) : ").strip()

    if choix == "1":
        # Mono-requ√™te
        query = input(
            "\n    Requ√™te de test (Entr√©e pour requ√™te par d√©faut) \n(Quels sont les dangers de l'intelligence artificielle ?)   :\n"
        ).strip()
        if not query:
            query = "Quels sont les dangers de l'intelligence artificielle ?"

        results = benchmark_single_query(query=query)

        if results:
            print_comparison_table(results, title=f"REQU√äTE: {query}")

            export = input("\nExporter ? (o/n) : ").strip().lower()
            if export == "o":
                export_results(results)

    elif choix == "2":
        # Multi-requ√™tes d√©faut
        all_results = benchmark_multiple_queries()

        if all_results:
            print_multi_query_summary(all_results)

            for query, results in all_results.items():
                print_comparison_table(results, title=f"REQU√äTE: {query[:50]}...")

            export = input("\nExporter ? (o/n) : ").strip().lower()
            if export == "o":
                export_multi_query_results(all_results)

    elif choix == "3":
        # Personnalis√©
        print("\nEntrez vos requ√™tes (une par ligne, ligne vide pour terminer) :")
        queries = []
        while True:
            q = input(f"Requ√™te {len(queries) + 1} : ").strip()
            if not q:
                break
            queries.append(q)

        if queries:
            all_results = benchmark_multiple_queries(queries=queries)

            if all_results:
                print_multi_query_summary(all_results)

                for query, results in all_results.items():
                    print_comparison_table(results, title=f"REQU√äTE: {query}")

                export = input("\nExporter ? (o/n) : ").strip().lower()
                if export == "o":
                    export_multi_query_results(all_results)

    elif choix == "4":
        print("\nAu revoir !")

    else:
        print("\nChoix invalide")


if __name__ == "__main__":
    interactive_menu()
